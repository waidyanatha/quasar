{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuaSaR: Identifying EEW Rings\n",
    "\n",
    "## GOAL AND OBJECTIVES\n",
    "Quake Safety Rings ([QuSaR](https://en.wikipedia.org/wiki/Quasar)) are essentially autonomous [Rings](https://brilliant.org/wiki/ring-theory/) of sensors sharing discretized time-series of _waveform_ information to identify threats and forewarn to give man and machine a lead time to respond to harmful earthquakes.\n",
    "\n",
    "The overall __gaol__ is to examine how the GeoNet seismic network can be augmented with a low-cost network to offer low-latency EEWs by making use of cutting-edge earthquake picking algorithms and machine learning techniques. The expected outcome is for the findings to serve as evidence for supporting a strategic deployment of a ring or rings of micro-array networks. \n",
    "\n",
    "The intent is to also make use of the analysis and tools is to serve as inputs for earthquake hazard risk assessment. Thereby, a community interested in operationalizing their own micro-array ring can make us of the analysis and tools to determining whether or not and how they may need to invest in building a micro-array ring.\n",
    "\n",
    "### Objectives\n",
    "1. _Understand the [topology](https://brilliant.org/wiki/topology/) (structure of connection of the units and their capabilities)_; also an axiomatic way to make sense of when two points in a set are \"near\" each other\n",
    "   1. Retrieve data on all the operational NZ seismic stations to __map the inventory__ by types and location [git-issue #2](https://github.com/waidyanatha/quasar/issues/2).\n",
    "   1. Build a __statoion fault topology space__ comprising all the operational stations within a bound of the fault line paths; such that we create a metric space _(X,d)_ comprising _X = { x,y | for all coordinate pairs of stations x and faults y}_ and a [haversine](https://math.stackexchange.com/questions/993236/calculating-a-perpendicular-distance-to-a-line-when-using-coordinates-latitude) distance function _d = x - y_; relative to the fault lines, stations, and earthquake detection role and capacity \n",
    "   1. Cluster the metric space into partially ordered __coarser topology__ of metric subspaces; essentially to make a nearest neigbour map of station fault clusters such that stations are within ___d < &epsilon;___ distance to ensure optimal EEW application performance; \n",
    "1. _Apply earthquake __picking algorithms__ on the GeoNet wave form data_\n",
    "   1. Test the __standard GeoNet algorithms__ (e.g. LTS/STS, Pd, )\n",
    "   1. Test with new __machine learning and wavefield algorithms__ (e.g. , 8bit Picking, PLUM)\n",
    "   1. Test above picking algorithms with __simulated earthquakes__ and for __selected high risk faults__ to observe the response of the picking algorithms\n",
    "   \n",
    "1. _Determine ways for improving the station rings for an incremental effectiveness of EEW_\n",
    "   1. Propose to __fit additional stations__ to improve the 30Km nearest neighbour cluster; then show how that improves the picking\n",
    "   1. Apply the geodedic methodology to __interpolate seismic data__ for the proposed station locations \n",
    "   1. Try the earthquake __picking algorithms__ on the hypothetical network to measure effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='../images/obspy.png',width=300, height=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEFINE data services and software modules\n",
    "\n",
    "We make use of the International Federation Data of Seismic Networks (FDSN), the global standard and a [data service](http://www.fdsn.org/services/) for sharing seismic sensor wave form data. The Obspy librarires support FDSN. The list of resources and services that are used for retrieving station inventory and waveform data.\n",
    "\n",
    "1. FDSN station service\n",
    "   1. FSDN as Client data sources; both (i) the FDSN client service and the (ii) FDSN complient GoeNet API webservice\n",
    "   1. retrieve station metadata information in a FDSN StationXML format or text format for all the channels in CECS station with no time limitations: https://service.geonet.org.nz/fdsnws/station/1/query?network=NZ&station=CECS&level=channel&format=text\n",
    "1. ObsPy\n",
    "   1. wavePicker is no longer supported by ObsPy; instead the [Pyrocko](http://pyrocko.org) Snuffler for seismic data inspection and picking is recommoended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from obspy import read_inventory\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.core import read, UTCDateTime\n",
    "#from datetime import date\n",
    "\n",
    "# Establish start and end time for retrieving waveform data\n",
    "t_start = UTCDateTime.now()-518400 #6 days ago = 60s x 60m x 24h x 6d\n",
    "t_end = UTCDateTime.now()+86400 #1 day in the future = 60s x 60m x 24h\n",
    "print('Station startime: ', t_start, '\\n & ending time: ', t_end)\n",
    "\n",
    "try:\n",
    "    #use either or GeoNet station service webservice URL or Obspy FDSN Client protocol to retrieve station data\n",
    "    st_ws = 'https://service.geonet.org.nz/fdsnws/station/1/query?network=NZ&level=station&endafter=2020-12-31&format=xml'\n",
    "    #st_ws = 'https://service.geonet.org.nz/fdsnws/station/1/query?network=NZ&station=CECS&level=channel'\n",
    "    # Set FDSN client URL to GEONET short code\n",
    "    client  = Client('GEONET')\n",
    "    print(\"Client is\",client)\n",
    "except Exception as err:\n",
    "    print(\"Error message:\", err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define station types and channels\n",
    "\n",
    "To learn about sensor type and channel code definitions [see section in ipynb](./stations_faultlnes_plot_1a.ipynb#sensor_code_desc)\n",
    "\n",
    "#### Class of station data processing methods\n",
    "The class is defined to manage all functions for retrieving, parsing, and preparing station data in an easily useable form.\n",
    "* Class _station_data()_\n",
    "   * _get_channels()_ returns abbreviated channel codes\n",
    "   * _get_types()_ returns a list of all seismic station types with abbreviation and description\n",
    "   * _get_stations()_ returns list of all stations with code, type abbr, lat/lon pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' All weak & strong motion, low gain, and mass possion sensor types '''\n",
    "class station_data():\n",
    "    def _init_(self, name):\n",
    "\n",
    "        name = \"station_metadata\"\n",
    "        return name\n",
    "        \n",
    "    def get_channels(self):\n",
    "        channels = \"UH*,VH*,LH*,BH*,SH*,HH*,EH*,UN*,VN*,LN*,BN*,SN*,HN*,EN*\"\n",
    "        return channels\n",
    "\n",
    "    '''\n",
    "        All combinations with definition of the first and second letter to define identify each station type\n",
    "    '''\n",
    "    def get_types(self):\n",
    "        dict_st_types = {\"UH\" : \"Weak motion sensor, e.g. measuring velocity\\nUltra Long Period sampled at 0.01Hz, or SOH sampled at 0.01Hz\",\n",
    "                   \"VH\" : \"Weak motion sensor, e.g. measuring velocity\\nVery Long Period sampled at 0.1Hz, or SOH sampled at 0.1Hz\",\n",
    "                   \"LH\" : \"Weak motion sensor, e.g. measuring velocity\\nBroad band sampled at 1Hz, or SOH sampled at 1Hz\",\n",
    "                   \"BH\" : \"Weak motion sensor, e.g. measuring velocity\\nBroad band sampled at between 10 and 80 Hz, usually 10 or 50 Hz\",\n",
    "                   \"SH\" : \"Weak motion sensor, e.g. measuring velocity\\nShort-period sampled at between 10 and 80 Hz, usually 50 Hz\", \n",
    "                   \"HH\" : \"Weak motion sensor, e.g. measuring velocity\\nHigh Broad band sampled at or above 80Hz, generally 100 or 200 Hz\",\n",
    "                   \"EH\" : \"Weak motion sensor, e.g. measuring velocity\\nExtremely Short-period sampled at or above 80Hz, generally 100 Hz\",\n",
    "                   \"UN\" : \"Strong motion sensor, e.g. measuring acceleration\\nUltra Long Period sampled at 0.01Hz, or SOH sampled at 0.01Hz\",\n",
    "                   \"VN\" : \"Strong motion sensor, e.g. measuring acceleration\\nVery Long Period sampled at 0.1Hz, or SOH sampled at 0.1Hz\",\n",
    "                   \"LN\" : \"Strong motion sensor, e.g. measuring acceleration\\nBroad band sampled at 1Hz, or SOH sampled at 1Hz\",\n",
    "                   \"BN\" : \"Strong motion sensor, e.g. measuring acceleration\\nBroad band sampled at between 10 and 80 Hz, usually 10 or 50 Hz\",\n",
    "                   \"SN\" : \"Strong motion sensor, e.g. measuring acceleration\\nShort-period sampled at between 10 and 80 Hz, usually 50 Hz\",\n",
    "                   \"HN\" : \"Strong motion sensor, e.g. measuring acceleration\\nHigh Broad band sampled at or above 80Hz, generally 100 or 200 Hz\",\n",
    "                   \"EN\" : \"Strong motion sensor, e.g. measuring acceleration\\nExtremely Short-period sampled at or above 80Hz, generally 100 Hz\"}\n",
    "        return dict_st_types\n",
    "\n",
    "    '''Prepare an array of station data: (i) station code as a unique identifier, \n",
    "                                        (ii) coordinates longitude & latitude, and \n",
    "                                       (iii) elevation in meters above mean sea level\n",
    "        return the construct as a list of stations including the list of invalid stations\n",
    "    '''\n",
    "    def get_stations(self):\n",
    "        st_list = []\n",
    "        invalid_st_list = []\n",
    "\n",
    "        try:\n",
    "            st_inv = client.get_stations(network='NZ', location=\"1?,2?\", station='*', channel=self.get_channels(), level='channel', starttime=t_start, endtime = t_end)\n",
    "        except Exception as err:\n",
    "            print(\"Error message:\", err)\n",
    "\n",
    "        '''run through stations to parse code, type, and location'''\n",
    "        try:\n",
    "            for each_st in range(len(st_inv[0].stations)):\n",
    "                ''' use lat/lon paris only in and around NZ remove all others '''\n",
    "                if(st_inv[0].stations[each_st].latitude < 0 and st_inv[0].stations[each_st].longitude > 0):\n",
    "                    each_st_type_dict = st_inv[0].stations[each_st].get_contents()\n",
    "                    ''' get the second character representing the station type '''\n",
    "#                    st_type_dict[\"st_type\"].append(each_st_type_dict[\"channels\"][0][-3:-1])\n",
    "                    ''' list of corresponding station locations (lat / lon) '''\n",
    "                    st_list.append([st_inv[0].stations[each_st].code, each_st_type_dict[\"channels\"][0][-3:-1], st_inv[0][each_st].latitude, st_inv[0][each_st].longitude])\n",
    "                else:\n",
    "                    '''dictionary of all stations not in NZ visinity '''\n",
    "                    invalid_st_list.append([st_inv[0].stations[each_st].code,st_inv[0][each_st].latitude, st_inv[0][each_st].longitude])\n",
    "\n",
    "        except Exception as err:\n",
    "            print(\"Error message:\", err)\n",
    "        \n",
    "        return st_list, invalid_st_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define fault lines\n",
    "\n",
    "#### Class of Fault line methods\n",
    "\n",
    "We have completed objective 1.A. However, we will also include a mapping of the fault lines to give a perception of the station distribution relative to that of the map of fault lines.\n",
    "\n",
    "* Class fault_data()\n",
    "   * _get_paths()_ to convert the WSG84 json file into a list\n",
    "   * _interpolate_paths_ input results from get_paths() and spcify an interpolation distance ( e.g. distance=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fault_data():\n",
    "\n",
    "    ''' TODO at initiatlization download latest ZIP'd datasets from GeoNet then extract the *.json\n",
    "    '''\n",
    "    def _init_(self):\n",
    "        pass\n",
    "\n",
    "    ''' Extract nested values from a JSON tree to build a list of fault lines\n",
    "        containing the fault name and lat / lon pairs of the path\n",
    "    '''\n",
    "    \n",
    "    def get_paths(self):\n",
    "        import json\n",
    "        from dictor import dictor\n",
    "        \n",
    "        try:\n",
    "            with open('../data/NZAFD/JSON/NZAFD_Oct_2020_WGS84.json') as json_file: \n",
    "                data = json.load(json_file)\n",
    "\n",
    "            faults = []\n",
    "            fault_path_count = 1\n",
    "            for each_feature in range(len(data['features'])):\n",
    "                flt = dictor(data,'features.{}.attributes.NAME'.format(each_feature))\n",
    "                if flt==\" \":\n",
    "                    flt = 'Unnamed fault '+ str(fault_path_count)\n",
    "                    fault_path_count += 1\n",
    "                points = []\n",
    "                path = dictor(data,'features.{}.geometry.paths.0'.format(each_feature))\n",
    "                for each_coordinate in range(len(path)):\n",
    "                    points.append([path[each_coordinate][0],path[each_coordinate][1]])\n",
    "                faults.append([flt,points])\n",
    "\n",
    "        except Exception as err:\n",
    "            print(\"Error message:\", err)\n",
    "        return faults\n",
    "\n",
    "    '''\n",
    "        Interpolate more points for each fault line; if the distance between points > 1.5Km @ 0.5Km intervals\n",
    "        Otherwise, fit a single halfway point\n",
    "    '''\n",
    "    def interpolate_paths(self, paths, distance=float(2.5)):\n",
    "        from shapely.geometry import LineString\n",
    "        \n",
    "        interp_paths = []\n",
    "        try:\n",
    "            ''' loop through each fault path to breakdown into line segments; i.e. coordinate pairs '''\n",
    "            for path in range(len(paths)):\n",
    "                path_index = 0\n",
    "                ''' add the two line segment coordinates to begin with\n",
    "                    now loop through each path line segment to add interpolated points  '''\n",
    "                while (path_index < len(paths[path][1])-1):\n",
    "                    ip = []     # interpolated point\n",
    "                    rel_origin_coord = paths[path][1][path_index]     # relative starting point of the path\n",
    "                    rel_nn_coord = paths[path][1][path_index+1]\n",
    "\n",
    "                    ''' change to a while loop until all distances between consecutive points < delta_distance'''\n",
    "                    while LineString([rel_origin_coord, rel_nn_coord]).length*6371.0 > distance:\n",
    "                        ip = LineString([rel_origin_coord,rel_nn_coord]).interpolate((10.0**3)/6371.0, normalized=True).wkt\n",
    "                        # convertion needs to happen otherwise throws an exception\n",
    "                        ip_lat = float(ip[ip.find(\"(\")+1:ip.find(\")\")].split()[0])\n",
    "                        ip_lon = float(ip[ip.find(\"(\")+1:ip.find(\")\")].split()[1])\n",
    "                        rel_nn_coord = list([ip_lat,ip_lon])\n",
    "                        ''' If you want to add the already interpolated coordinates to the path to possibly speedup\n",
    "                        and use those points to create a denser path; note that it may will results in uniequal\n",
    "                        distant between consecutive points in the path. Comment the instruction below to disable.\n",
    "                        '''\n",
    "                        paths[path][1].insert(path_index+1,rel_nn_coord)    # interpolated coordinates closest to the relative origin\n",
    "\n",
    "                    path_index += 1\n",
    "\n",
    "                interp_paths.append([paths[path][0], paths[path][1]])\n",
    "\n",
    "        except Exception as err:\n",
    "            print(\"Error message:\", err)\n",
    "\n",
    "        return interp_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJECTIVE 1.B - STATION FAULT METRIC\n",
    "\n",
    "### Data preperation for analysis\n",
    "The steps below build a set of list and array metric for the stations and fault lines:\n",
    "1. Interpolate points between fault line path coordinates\n",
    "1. Calculate the station to fault line perpendicular distances\n",
    "\n",
    "#### Why interpolate more coordinates?\n",
    "The fault line paths might have been reduced by applying the [Ramer-Douglus-Peuker algotithm](https://pypi.org/project/rdp/) before publishing the GeoNet fault line paths with an optimal set of coordinates sufficient for mapping - _Edward Lee pointed out that instead of using the \"perpendicular distance\" from a point to a line, the algorithm should use the 'Shortest Distance' from a point to a line segment._ Therefore, we are essentially inverting the rdp PyPi agoritm to interpolate more coordinates to reduce the line segment lengths to ~1.0 Km.\n",
    "\n",
    "#### Interpolate coordinates in ~1.0Km separations\n",
    "The average distance between consecutive coordinates in each fault line path latitude and longitude pairs range from 2.0 - 30.0 Km. Therefore; we use the [shapely interpolation](https://shapely.readthedocs.io/en/latest/manual.html#linear-referencing-methods) techniques to synthesize coordinates such that the distance between consecutive coordinates is ~ 1.0 Km.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Interpolation method - to add more lat/lon coordinates between fault line segments\n",
    "'''\n",
    "import sys\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "try:\n",
    "    faults = fault_data()     # declare fault lines class\n",
    "    original_paths = faults.get_paths()     # get all fault line paths\n",
    "\n",
    "    ''' analyse the distance between fault line path coordinates '''\n",
    "    print(\"Statistics of {} original fault lines before interpolating\".format(len(original_paths)))\n",
    "    for path in range(len(original_paths)):\n",
    "        sum_lengths = float(0)\n",
    "        for coords in range(len(original_paths[path][1])-1):\n",
    "            sum_lengths += LineString([original_paths[path][1][coords], \n",
    "                                       original_paths[path][1][coords+1]]).length*6371.0 \n",
    "#        print(\"{0} has {1} coordinates with an average inter-coordinate distance: {2} Km\".format(original_paths[path][0], len(original_paths[path][1]), str(sum_lengths/len(original_paths[path][1]))))\n",
    "        sys.stdout.write(\"{0} has {1} coordinates with an average inter-coordinate distance: {2} Km\".format(original_paths[path][0], len(original_paths[path][1]), str(sum_lengths/len(original_paths[path][1]))))\n",
    "    \n",
    "    interpolated_paths = faults.interpolate_paths(paths=original_paths,distance=2.5)\n",
    "    print(\"\\nWait until interpolation is complete ...\")\n",
    "    print(\"\\nPost interpolation statistics of {} fault lines\".format(len(interpolated_paths)))\n",
    "    for path in range(len(interpolated_paths)):\n",
    "        sum_lengths = float(0)\n",
    "        for coords in range(len(interpolated_paths[path][1])-1):\n",
    "            sum_lengths += LineString([interpolated_paths[path][1][coords], \n",
    "                                   interpolated_paths[path][1][coords+1]]).length*6371.0 \n",
    "#        print(\"{0} has {1} coordinates with an average inter-coordinate distance: {2} Km\".format(interpolated_paths[path][0], len(interpolated_paths[path][1]), str(sum_lengths/len(interpolated_paths[path][1]))))\n",
    "        sys.stdout.write(\"{0} has {1} coordinates with an average inter-coordinate distance: {2} Km\".format(interpolated_paths[path][0], len(interpolated_paths[path][1]), str(sum_lengths/len(interpolated_paths[path][1]))))\n",
    "    '''TODO change output to give numbers only; e.g. mean, median, and variance of fault coordinate distances'''\n",
    "        \n",
    "    '''TODO write the non-empty interpolated dataset to a file'''\n",
    "#    if :\n",
    "#        with open('../data/NZAFD/JSON/interpolated_NZAFD_Oct_2020_WGS84.json', 'w') as outfile:\n",
    "#            json.dump(interpolated_paths, outfile)\n",
    "\n",
    "except Exception as err:\n",
    "    print(\"Error message:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Station to nearest fault line distance metric\n",
    "Estimate distance from station to nearest fault line segment. Thereafter, associate each station with the nearest neigbour fault line segments. We have a station with coordinates _A=\\[s_lat, s_lon\\]_ and two coordinates _B=\\[f1_lat,f1_lon\\]_ and _C=\\[f2_lat, f2_lon\\]_, and want to project A onto the arc between B and C, and find the length of the projection arc. \n",
    "\n",
    "1. __Loop through stations and faults__ to build a distance metric that can be used to determine the station sequence that might be triggered by a particular earthquke from a location along a fault line\n",
    "1. Ideally __calculate perpendicular distance__ from the station to the line segment; i.e. [shortest arc length](https://math.stackexchange.com/questions/993236/calculating-a-perpendicular-distance-to-a-line-when-using-coordinates-latitude)\n",
    "    1. _Compute_ `n=A×B` (\"×\" the cross product) and `N=n/√n⋅n` (\"⋅\" the dot product)\n",
    "    1. _Convert the coordinates_ A, B, & C to _\\[x,y,z\\]_ triples with `x=sinucosv; y=sinv; z=cosucosv`\n",
    "    1. _Compute_ the angular distance between \n",
    "        1. a ray from the earth's center to A and the plane _n_ described above `s=90∘−|arccos(C⋅N)|`\n",
    "        1. the \"distance\" between A and B as `s′=arccos(A⋅B)`; assuming working in degrees (range from 0 to 180)\n",
    "1. For now, differ to __calculate the shortest distance__ recommended by Edward Lee discussed in [why we  interpolate?](#Why-interpolate-more-coordinates?) \n",
    "1. \\[ERROR grumbling about lat / lon attributes\\] __Obspy geodedics__ [inside_geobounds](https://docs.obspy.org/packages/autogen/obspy.geodetics.base.inside_geobounds.html#obspy.geodetics.base.inside_geobounds) function can confirm whether the fault line segments A-B are within a given radius of the station A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from obspy.geodetics import base\n",
    "from shapely.geometry import LineString\n",
    "import sys\n",
    "\n",
    "def get_station_fault_metric_list():\n",
    "#    try:\n",
    "    st_meta = station_data()\n",
    "    st_list, invalid_st_list = st_meta.get_stations()\n",
    "    print('There are {0} active valid stations and {1} invalid station(s)'.format(len(st_list),len(invalid_st_list)))\n",
    "    print('The invalid stations are:{0}'.format(invalid_st_list))\n",
    "    print('Unique station types 1st & 2nd letters of station codes are: {})'.format(set(item[1] for item in st_list)))        \n",
    "\n",
    "#    except Exception as err:\n",
    "#        print(\"Error message:\", err)\n",
    "\n",
    "    #st_meta = station_data()\n",
    "    #st_list, invalid_st_list = st_meta.get_stations()\n",
    "\n",
    "    '''\n",
    "        move along each fault line coordinates to find a station closest to that point withing a 30Km radius.\n",
    "    '''\n",
    "    try:\n",
    "        st_flt_metric = []\n",
    "        short_dist_ub = float(10**4)\n",
    "        null_nearest_flt_coord = [0.0000, 0.0000]\n",
    "\n",
    "        print(\"Wait for a few minutes to build the metric comprising {} stations and {} faults...\".format(len(st_list), len(interpolated_paths)))\n",
    "        for indx, each_station in enumerate(st_list):\n",
    "            sys.stdout.write(\"\\r\" + \"{0} of {1} Calculating faults closest to Station {2}.\".format(indx+1, len(st_list), each_station[0]))\n",
    "   #         print(\"{0} Calculating faults closest to Station {1} latitude {2} and longitude {3}\".format(indx+1, each_station[0],each_station[2],each_station[3]))\n",
    "\n",
    "            for each_fault in interpolated_paths:\n",
    "                st_coord = [each_station[3],each_station[2]]\n",
    "                shortest_distance = short_dist_ub\n",
    "                nearest_fault_coord = null_nearest_flt_coord\n",
    "                for flt_coord in range(len(each_fault[1])):\n",
    "                    st_to_flt = LineString([each_fault[1][flt_coord], st_coord]).length*6371.0\n",
    "\n",
    "                    ''' TODO make the correct projection\n",
    "                    st_to_flt = LineString([each_fault[1][flt_coord], st_coord])\n",
    "                    st_to_flt.srid = 4326\n",
    "                    st_to_flt.transform(3857)\n",
    "                    st_to_flt.length\n",
    "                    '''\n",
    "                    if st_to_flt < shortest_distance:\n",
    "                        shortest_distance = st_to_flt\n",
    "                        nearest_fault_coord = each_fault[1][flt_coord]\n",
    "                if shortest_distance < short_dist_ub :\n",
    "                    shortest_distance = shortest_distance  \n",
    "                    st_flt_metric.append([each_station[0], st_coord, each_fault[0], nearest_fault_coord, shortest_distance])\n",
    "    \n",
    "            '''\n",
    "                TODO fix the error on the lat / lon attributes\n",
    "                if base.inside_geobounds(each_fault[1], minlatitude=None, maxlatitude=None, \n",
    "                                         minlongitude=None, maxlongitude=None, \n",
    "                                         latitude=36, longitude=174, \n",
    "                                 minradius=1/6378137.0, maxradius=30.0/6378137):\n",
    "                    print(each_fault[0],\"yes\")\n",
    "                else:\n",
    "                    print(each_fault[0],\"no\")\n",
    "            '''\n",
    "        print(\"Done building the metric size {}\".format(len(st_flt_metric))  if len(st_flt_metric) > 0 else \"Empty metric; no data was built\")\n",
    "\n",
    "        #            min_distance_to_fault = calc_vincenty_inverse(lat1, lon1, lat2, lon2, a=6378137.0, f=0.0033528106647474805)\n",
    "        #            statio_faults.append[interpolated_paths[each_fault]]\n",
    "    except Exception as err:\n",
    "        print(\"Error message:\", err)\n",
    "        \n",
    "    return st_flt_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a 2D array station-fault metric\n",
    "Begins with the non-empty set station fault list comprising the station code and coordinates, fault name and coordinates, and the distance between them. The list is transformed into a n_station by n_fault 2D array with element values:\n",
    "* _r\\_station\\_type_ - a ranking of the [station types](#Class-of-station-data-processing-methods) based on their contribution to earthquake detection\n",
    "* _d\\_station\\_fault_ - distance between the station coordinates and the nearest interpolated fault path coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_fault_metric_array(list_st_flt_metric: list, max_separation: float = 30000.0):\n",
    "    import numpy as np\n",
    "\n",
    "    if not isinstance(list_st_flt_metric, list):\n",
    "        raise TypeError\n",
    "#    return list_st_flt_metric[index]\n",
    "    \n",
    "    count = 0\n",
    "    st_flt_arr = np.array([],[])\n",
    "    st_flt_ub = max_separation     # 30Km distance between station and fault line\n",
    "    cls_st = station_data()  # from the class 'station data' get dictionary of 'station types'\n",
    "    lst_tmp_st_types = []\n",
    "    for idx_st_type, val_st_type in enumerate(list(cls_st.get_types())):\n",
    "        lst_tmp_st_types.append([idx_st_type, val_st_type])\n",
    "\n",
    "    try:\n",
    "        '''\n",
    "            filter list to refelct maximum separation upper bound for the distance between faults and stations\n",
    "        '''\n",
    "        \n",
    "        bounded_st_flt = [idx for idx, element in enumerate([row[4] for row in list_st_flt_metric]) if element <= st_flt_ub]\n",
    "        print(\"\\nNumber of stations and faults within {0}m distance {1} of a total {2}\".format(st_flt_ub,len(bounded_st_flt), len(list_st_flt_metric)))\n",
    "\n",
    "        '''\n",
    "            Build the input array with rows = station and columns = faults\n",
    "        '''\n",
    "        unique_stations = set([row[0] for row in list_st_flt_metric])\n",
    "        unique_faults = set([row[2] for row in list_st_flt_metric])\n",
    "        st_flt_arr = np.zeros([len(unique_stations),len(unique_faults)], dtype = float)\n",
    "        tmp_st_flts = []\n",
    "        print(\"Wait a moment while we construct an array with shape {} for stations in rows and faults in columns\".format(st_flt_arr.shape))\n",
    "\n",
    "        '''\n",
    "            TODO change the array element to a tuple with [station-type-ranking, station-fault-distance]\n",
    "        '''\n",
    "        for st_indx, st_val in enumerate(unique_stations):\n",
    "            # retrieve the station code, fault name, and distance from the list\n",
    "            tmp_st_flts = [[row[0],row[2],row[4]] for row in list_st_flt_metric if row[0] == st_val]\n",
    "            tmp_st_type = [row[0] for row in lst_tmp_st_types if row[1] == str(st_val[1]+st_val[2])]\n",
    "            for flt_indx, flt_val in enumerate(unique_faults):\n",
    "                # from the st_flt_metric get the distance value corresponding to the st_val and flt_val\n",
    "                for tmp_indx, row in enumerate(tmp_st_flts):\n",
    "                    if  row[0] == st_val and row[1] == flt_val:\n",
    "                        st_flt_arr[st_indx,flt_indx] = [tmp_st_type,row[2]]\n",
    "\n",
    "        ''' TODO remove all zero rows and columns '''\n",
    "        #st_flt_arr[~np.all(st_flt_arr == 0, axis=0)]\n",
    "        #st_flt_arr[~np.all(st_flt_arr[..., :] == 0, axis=0)]\n",
    "        print(\"station fault {1}D array shape {0} has {2} elements and an itemsize {3}\".format(st_flt_arr.shape, st_flt_arr.ndim, st_flt_arr.size, st_flt_arr.itemsize))\n",
    "    except Exception as err:\n",
    "        print(\"Error message:\", err)\n",
    "    return st_flt_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJECTIVE 1.C - STATION FAULT COARSEST TOPOGRAPHY\n",
    "\n",
    "### Define clustering methods\n",
    "[Learn about clustering methods](https://realpython.com/k-means-clustering-python/)\n",
    "#### Class of Clustering algorithms\n",
    "1. _get_dbscan_labels()_\n",
    "    1. Compute the cluster property measures to estimate the acceptability\n",
    "    1. Dump the output to a file including cluster label, lat/lon, station code, and so on\n",
    "1. _get_nn_labels()_\n",
    "    1. Compute the mean distance between [nearest neigbours](https://scikit-learn.org/stable/modules/neighbors.html) of a minimum 3 points\n",
    "    1. Also consider [mean nearest neighbour distance](https://pysal.org/notebooks/explore/pointpats/distance_statistics.html#Mean-Nearest-Neighbor-Distance-Statistics)\n",
    "1. _get_kmean_labels()_\n",
    "    1. separates the station fault distances into _n\\_clusters_ with similar variances from the mean centroid\n",
    "    1. returns the cluster labels associated with the station fault metric\n",
    "\n",
    "__Note 1:__ - Apply DBSCAN to cluster stations with an epsilon < 30Km. DBSCAN is preferred over K-means clustering because K-means clustering considance the variance while DBSCAN considers a distance function. It gives the capacity to build clusters serving the criteria of < 30Km distance between stations.\n",
    "\n",
    "__Note 2:__ - Inherent __problem of DBSCAN__ is that it characterises data points to be in the same clusted if pair-wise data points satisfy the epsilon condition. This would not adequately satisfy the required condition that all data points in a a cluster are within the desired epsilon distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Relevant clustering functions necessary for the station-fault analysis\n",
    "'''\n",
    "class clustering():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    '''\n",
    "        TODO consider OPTICS (Ordering Points To Identify the Clustering Structure)\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "        DBSCAN clustering - lat/lon pairs\n",
    "    '''\n",
    "    def get_dbscan_labels(self,st_arr):\n",
    "    \n",
    "        from sklearn.cluster import DBSCAN\n",
    "        from sklearn import metrics\n",
    "        import sklearn.utils\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.datasets import make_blobs\n",
    "\n",
    "        err=\"0\"\n",
    "    #    try:\n",
    "        X, labels_true = make_blobs(n_samples=len(st_arr), centers=st_arr, cluster_std=0.4,random_state=0)\n",
    "        db = DBSCAN(eps=30.0/6371.0, min_samples=3, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "        print('DBSCAN epsilon:',db.eps,'algorithm:', db.algorithm, 'metric: ', db.metric)\n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "#        print('core samples mask', len(core_samples_mask),core_samples_mask)\n",
    "        labels = db.labels_\n",
    "#        print(\"DBSCAN found %0.3f labels\" % labels )\n",
    "    #    except Exception as err:\n",
    "    #        print(\"Error message:\", err)\n",
    "    #        labels = \"\"\n",
    "        return labels, labels_true, core_samples_mask\n",
    "\n",
    "    '''\n",
    "        K nearest neigbour clustering\n",
    "    '''\n",
    "    def get_nn_labels(self,st_flt_list):\n",
    "    \n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "        # Augment station array with cluster number\n",
    "        # Start a new station coorinates and details tuple\n",
    "        st_list = []\n",
    "        i=0\n",
    "        for i in range(len(labels)):\n",
    "            st_row = [tmp_arr[i,0],labels[i],tmp_arr[i,1],tmp_arr[i,2],tmp_arr[i,3]]\n",
    "            st_list.append(list(st_row))\n",
    "\n",
    "        clusters = list({item[1] for item in st_list})\n",
    "\n",
    "        for each_cluster in clusters:\n",
    "            cluster_list = list(st_list[j] for j in range(len(st_list)) if st_list[j][1] == each_cluster)\n",
    "            cluster_arr = np.delete(cluster_list, [0,1,4],axis=1).astype(np.float)\n",
    "            nbrs = NearestNeighbors(n_neighbors=3, algorithm='brute', metric='haversine').fit(cluster_arr)\n",
    "            distances, indices = nbrs.kneighbors(cluster_arr)\n",
    "            print(nbrs.kneighbors_graph(cluster_arr).toarray())\n",
    "    \n",
    "            each_cluster_clique = client.get_stations(latitude=-42.693,longitude=173.022,maxradius=30.0/6371.0, starttime = \"2016-11-13 11:05:00.000\",endtime = \"2016-11-14 11:00:00.000\")\n",
    "            print(each_cluster_clique)\n",
    "            _=inventory.plot(projection=\"local\")\n",
    "    \n",
    "            break\n",
    "\n",
    "        sorted_rank = sorted(st_list, key=lambda i: (int(i[1])), reverse=True)\n",
    "        #print('Code, Cluster, Latitude, Longitude, Elevation')\n",
    "        #print(sorted_rank)\n",
    "        return sorted_rank\n",
    "    \n",
    "    '''\n",
    "        K Means clustering - station-fault distance metric\n",
    "        Parameters:\n",
    "            number of clusters = 5 gives optimal Homogeneity, V-measure, and Silhouette Coefficient\n",
    "            maximum number of iterations = 300 to minimize clustering quality; i.e. sum of the squared error\n",
    "    '''\n",
    "    def get_kmean_labels(self, st_flt_arr, num_clusters=5):\n",
    "        \n",
    "        from sklearn.cluster import KMeans\n",
    "#        import sklearn.utils\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        from sklearn.datasets import make_blobs\n",
    "        import numpy as np\n",
    "        \n",
    "        st_flt_arr.reshape(-1, 1)\n",
    "        X, labels_true = make_blobs(n_samples=len(st_flt_arr), centers=st_flt_arr, cluster_std=0.4,random_state=0)\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(X)\n",
    "#        kmeans = KMeans(init=\"random\", n_clusters=num_clusters, n_init=5,max_iter=300, random_state=5).fit(X)\n",
    "        kmeans = KMeans(init=\"random\", n_clusters=num_clusters, n_init=5,max_iter=300, random_state=5)\n",
    "        kmeans.fit(scaled_features)\n",
    "        labels = kmeans.labels_\n",
    "        print(kmeans.labels_)\n",
    "        core_samples_mask = np.zeros_like(kmeans.labels_, dtype=bool)\n",
    "#        core_samples_mask[kmeans.core_sample_indices_] = True\n",
    "        \n",
    "#        return kmeans\n",
    "        return labels, labels_true, core_samples_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Stations and faults by distance\n",
    "\n",
    "#### Apply K-means clustering\n",
    "We use the k-means function defined in the [clustering class](#Class-of-Clustering-algorithms). There are several [drawbacks SciKit preassumes](https://scikit-learn.org/stable/modules/clustering.html#k-means) that have been considered on the assumption that the clusters are convex and isotropic and a principle component analysis has been applied prior to the clustering. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "    reconstruct the station fault metric list to an array\n",
    "'''\n",
    "try:\n",
    "    st_flt_arr = np.array([],[])\n",
    "    print(\"Wait a moment to construct the station fault metric list ...\")\n",
    "#    st_flt_list = get_station_fault_metric_list()\n",
    "    if not isinstance(st_flt_list, list):\n",
    "        raise TypeError\n",
    "    else:\n",
    "        print(\"Received station fault list with distance metric and it looks like this:\\n{}\".format(st_flt_list[0:5]))\n",
    "        st_flt_arr = get_station_fault_metric_array(st_flt_list)\n",
    "        print(\"Received array with {0} dimensions of shape {1} and it looks like this:\\n{2}\".format(st_flt_arr.ndim, st_flt_arr.shape,st_flt_arr))\n",
    "except Exception as err:\n",
    "    print(\"Error message:\", err)\n",
    "\n",
    "''' \n",
    "    Apply k-means clustering to the 2D array metric\n",
    "'''\n",
    "try:\n",
    "    cl_method = clustering()\n",
    "    # Run k means to get the cluster labels\n",
    "    print(\"Begin k means clustering ...\")\n",
    "    labels, labels_true, core_samples_mask = cl_method.get_kmean_labels(st_flt_arr, 13)\n",
    "    #print('core samples mask', len(core_samples_mask),core_samples_mask)\n",
    "    print(\"Clustering complete!\")\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "\n",
    "    print('\\nPerformance evaluation ...')\n",
    "    print('Total number of stations: %d' % len(labels))\n",
    "    print('Estimated number of clusters: %d' % n_clusters_)\n",
    "    print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "    print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "    print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "    print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "    print(\"Adjusted Rand Index: %0.3f\"\n",
    "          % metrics.adjusted_rand_score(labels_true, labels))\n",
    "    print(f\"Adjusted Mutual Information: %0.3f\" % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "    print(\"Silhouette Coefficient: %0.3f\"\n",
    "          % metrics.silhouette_score(st_flt_arr, labels))\n",
    "\n",
    "except Exception as err:\n",
    "    print(\"Error message:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results\n",
    "1. Plot clusters as [Voroni Cells](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.voronoi_plot_2d.html) with varied colors unique to each cluster and also displaying the centroid\n",
    "1. plot fault lines to show closes sensor in cluster to the fault line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "plt.figure(figsize=(30, 40))\n",
    "#nz_map = Basemap(width=15000,height=15000,projection='merc',\n",
    "#            resolution='l',lat_0=-40,lon_0=176.)\n",
    "#nz_map.drawcoastlines()\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = station_coordinates[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    # uncomment to plot the noise\n",
    "    #xy = station_coordinates[class_member_mask & ~core_samples_mask]\n",
    "    #plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "    #         markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.legend(loc='upper left', fontsize=20)\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISCUSSION\n",
    "\n",
    "## Data preperation\n",
    "\n",
    "## Clustering\n",
    "\n",
    "### DBSCAN results\n",
    "It is evident from the cluster with large volume of data points are spread across the geography. Therefore, DBSCAN is shown to be innopriate for clustering stations to estimate whether they hold the property of being 30Km within each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESOURCES\n",
    "1. [Global data services and standards](http://www.fdsn.org/services/) offered by the International Federation Data of Seismic Networks (FDSN). \n",
    "1. GEONET resources:\n",
    "   1. [Stream Naming Conventions](https://www.geonet.org.nz/data/supplementary/channels) are based on historical usage together with recommendations from the [SEED manual](https://www.fdsn.org/seed_manual/SEEDManual_V2.4.pdf)\n",
    "   1. [Python tutorials](https://www.geonet.org.nz/data/tools/Tutorials) for using GeoNet resources\n",
    "1. [Seismo-Live](https://krischer.github.io/seismo_live_build/html/Workshops/2017_Baku_STCU_IRIS_ObsPy_course/07_Basic_Processing_Exercise_solution_wrapper.html) examples of get station waveform, inventory, event, arrival time, response, and plotting using obspy\n",
    "1. Choosing [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) over KMeans: \n",
    "   1. Discussion of the [three clustering methods](https://realpython.com/k-means-clustering-python/): K means, hierachical, and density-based clustering\n",
    "   1. Fundermentally KMeans requires us to first select the number of clusters we wish to find and DBSCAN doesn't.\n",
    "   1. [clustering to reduce spatial data sizes](https://geoffboeing.com/2014/08/clustering-to-reduce-spatial-data-set-size/) KMeans is not an ideal algorithm for latitude-longitude spatial data because it minimizes variance, not geodetic distance. \n",
    "   1. [Explanation of DBSCAN clustering](https://towardsdatascience.com/explaining-dbscan-clustering-18eaf5c83b31) also identifies a drawback of KMeans clustering as it is vulnerable to outliers and outliers have a significant impact on the way the centroids moves.\n",
    "1. [Example of scikit-learn DBSCAN](https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html)\n",
    "1. [obspy.geodetics](https://docs.obspy.org/packages/obspy.geodetics.html) - various geodetic utilities for ObsPy - try an alternative clustering method with obspy geodetics\n",
    "1. Mapping tutorials\n",
    "   1. Visualization: [Mapping Global Earthquake Activity](http://introtopython.org/visualization_earthquakes.html)\n",
    "   1. Plotting data on a map [(Example Gallery)](https://matplotlib.org/basemap/users/examples.html)\n",
    "1. Calculating a [perpendicular distance to a line](https://math.stackexchange.com/questions/993236/calculating-a-perpendicular-distance-to-a-line-when-using-coordinates-latitude), when using coordinates (latitude & longitude)\n",
    "1. Apply the [moment tensor](https://earthquake.usgs.gov/learn/glossary/?term=moment%20tensor); especially, the Seismic Moment Tensor Inversion (SMTI) analysis when computing fault line movement and picking the stations that might be first triggered because the [moment tensor would determine the intensity and wave propogation characteristics](https://www.esgsolutions.com/technical-resources/microseismic-knowledgebase/what-is-a-moment-tensor). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
