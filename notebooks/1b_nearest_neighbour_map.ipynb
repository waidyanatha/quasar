{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuaSaR: Identifying EEW Rings\n",
    "\n",
    "## GOAL AND OBJECTIVES\n",
    "Quake Safety Rings ([QuSaR](https://en.wikipedia.org/wiki/Quasar)) are essentially autonomous [Rings](https://brilliant.org/wiki/ring-theory/) of sensors sharing discretized time-series of wave form information to identify threats and forewarn to give man and machine a lead time to respond to harmful earthquakes.\n",
    "\n",
    "The overall __gaol__ is to examine how the GeoNet seismic network can be augmented with a low-cost network to offer low-latency EEWs by making use of cutting-edge earthquake picking algorithms and machine learning techniques. The expected outcome is for the findings to serve as evidence for supporting a strategic deployment of a ring or rings of micro-array networks. \n",
    "\n",
    "The intent is to also make use of the analysis and tools is to serve as inputs for earthquake hazard risk assessment. Thereby, a community interested in operationalizing their own micro-array ring can mae us of the analysis and tools to determining whether or not and how they may need to invest in building a micro-array ring.\n",
    "\n",
    "### Objectives\n",
    "1. _Understand the network dynamics (structure of connection of the units and their capabilities)_\n",
    "   1. Retrieve data on all the operational NZ seismic stations to __map the inventory__ by types and location.\n",
    "   1. Build a __nearest neigbour map of clusters__ of all the operational stations within a 30Km radius\n",
    "   1. Determine station cluster __topography__ relative to the __fault lines__ and earthquake detection role and capacity  \n",
    "1. _Apply earthquake __picking algorithms__ on the GeoNet wave form data_\n",
    "   1. Test the __standard GeoNet algorithms__ (e.g. LTS/STS, Pd, )\n",
    "   1. Test with new __machine learning and wavefield algorithms__ (e.g. , PLUM)\n",
    "   1. Test above picking algorithms with __simulated earthquakes__ and for __selected high risk faults__ to observe the response of the picking algorithms\n",
    "   \n",
    "1. _Determine ways for improving the station rings for an incremental effectiveness of EEW_\n",
    "   1. Propose to __fit additional stations__ to improve the 30Km nearest neighbour cluster; then show how that improves the picking\n",
    "   1. Apply the geodedic methodology to __interpolate seismic data__ for the proposed station locations \n",
    "   1. Try the earthquake __picking algorithms__ on the hypothetical network to measure effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJECTIVE 1.B - NEAREST NEIGBOUR MAP\n",
    "\n",
    "### DEFINE data services and software modules\n",
    "\n",
    "We make use of the International Federation Data of Seismic Networks (FDSN), the global standard and a [data service](http://www.fdsn.org/services/) for sharing seismic sensor wave form data. The Obspy librarires support FDSN. The list of resources and services that are used for retrieving station inventory and waveform data. \n",
    "1. ObsPy\n",
    "   1. FSDN as Client data sources; both (i) the FDSN client service and the (ii) FDSN complient GoeNet API webservice\n",
    "   1. Core to read data and utilize datatime features\n",
    "   1. Picking algorithms\n",
    "1. FDSN station service\n",
    "   1. retrieve station metadata information in a FDSN StationXML format or text format for all the channels in CECS station with no time limitations: https://service.geonet.org.nz/fdsnws/station/1/query?network=NZ&station=CECS&level=channel&format=text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from obspy import read_inventory\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.core import read, UTCDateTime\n",
    "#from datetime import date\n",
    "\n",
    "# Establish start and end time for retrieving waveform data\n",
    "t_start = UTCDateTime.now()-518400 #6 days ago = 60s x 60m x 24h x 6d\n",
    "t_end = UTCDateTime.now()+86400 #1 day in the future = 60s x 60m x 24h\n",
    "print('Station startime: ', t_start, '\\n & ending time: ', t_end)\n",
    "\n",
    "try:\n",
    "    #use either or GeoNet station service webservice URL or Obspy FDSN Client protocol to retrieve station data\n",
    "    st_ws = 'https://service.geonet.org.nz/fdsnws/station/1/query?network=NZ&level=station&endafter=2020-12-31&format=xml'\n",
    "    #st_ws = 'https://service.geonet.org.nz/fdsnws/station/1/query?network=NZ&station=CECS&level=channel'\n",
    "    # Set FDSN client URL to GEONET short code\n",
    "    client  = Client('GEONET')\n",
    "    print(\"Client is\",client)\n",
    "except Exception as err:\n",
    "    print(\"Error message:\", err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define station types and channels\n",
    "\n",
    "To learn about sensor type and channel code definitions [see section in ipynb](./stations_faultlnes_plot_1a.ipynb#sensor_code_desc)\n",
    "\n",
    "#### Class of station data processing methods\n",
    "The class is defined to manage all functions for retrieving, parsing, and preparing station data in an easily useable form.\n",
    "* Class _station_data()_\n",
    "   * _get_channels()_ returns abbreviated channel codes\n",
    "   * _get_types()_ returns a list of all seismic station types with abbreviation and description\n",
    "   * _get_stations()_ returns list of all stations with code, type abbr, lat/lon pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' All weak & strong motion, low gain, and mass possion sensor types '''\n",
    "class station_data():\n",
    "    def _init_(self, name):\n",
    "\n",
    "        name = \"station_metadata\"\n",
    "        return name\n",
    "        \n",
    "    def get_channels(self):\n",
    "        channels = \"UH*,VH*,LH*,BH*,SH*,HH*,EH*,UN*,VN*,LN*,BN*,SN*,HN*,EN*\"\n",
    "        return channels\n",
    "#channels = \"UH*,VH*,LH*,BH*,SH*,HH*,EH*,UN*,VN*,LN*,BN*,SN*,HN*,EN*\"\n",
    "\n",
    "    def get_types(self):\n",
    "        st_types = {\"UH\" : \"Weak motion sensor, e.g. measuring velocity\\nUltra Long Period sampled at 0.01Hz, or SOH sampled at 0.01Hz\",\n",
    "                   \"VH\" : \"Weak motion sensor, e.g. measuring velocity\\nVery Long Period sampled at 0.1Hz, or SOH sampled at 0.1Hz\",\n",
    "                   \"LH\" : \"Weak motion sensor, e.g. measuring velocity\\nBroad band sampled at 1Hz, or SOH sampled at 1Hz\",\n",
    "                   \"BH\" : \"Weak motion sensor, e.g. measuring velocity\\nBroad band sampled at between 10 and 80 Hz, usually 10 or 50 Hz\",\n",
    "                   \"SH\" : \"Weak motion sensor, e.g. measuring velocity\\nShort-period sampled at between 10 and 80 Hz, usually 50 Hz\", \n",
    "                   \"HH\" : \"Weak motion sensor, e.g. measuring velocity\\nHigh Broad band sampled at or above 80Hz, generally 100 or 200 Hz\",\n",
    "                   \"EH\" : \"Weak motion sensor, e.g. measuring velocity\\nExtremely Short-period sampled at or above 80Hz, generally 100 Hz\",\n",
    "                   \"UN\" : \"Strong motion sensor, e.g. measuring acceleration\\nUltra Long Period sampled at 0.01Hz, or SOH sampled at 0.01Hz\",\n",
    "                   \"VN\" : \"Strong motion sensor, e.g. measuring acceleration\\nVery Long Period sampled at 0.1Hz, or SOH sampled at 0.1Hz\",\n",
    "                   \"LN\" : \"Strong motion sensor, e.g. measuring acceleration\\nBroad band sampled at 1Hz, or SOH sampled at 1Hz\",\n",
    "                   \"BN\" : \"Strong motion sensor, e.g. measuring acceleration\\nBroad band sampled at between 10 and 80 Hz, usually 10 or 50 Hz\",\n",
    "                   \"SN\" : \"Strong motion sensor, e.g. measuring acceleration\\nShort-period sampled at between 10 and 80 Hz, usually 50 Hz\",\n",
    "                   \"HN\" : \"Strong motion sensor, e.g. measuring acceleration\\nHigh Broad band sampled at or above 80Hz, generally 100 or 200 Hz\",\n",
    "                   \"EN\" : \"Strong motion sensor, e.g. measuring acceleration\\nExtremely Short-period sampled at or above 80Hz, generally 100 Hz\"}\n",
    "        return st_types\n",
    "    \n",
    "    def get_stations(self):\n",
    "\n",
    "        try:\n",
    "            st_inv = client.get_stations(network='NZ', location=\"1?,2?\", station='*', \n",
    "                                         channel=self.get_channels(), level='channel', \n",
    "                                         starttime=t_start, endtime = t_end)\n",
    "\n",
    "            st_list = []\n",
    "            invalid_st_list = []\n",
    "            '''run through stations to parse code, type, and location'''\n",
    "            for each_st in range(len(st_inv[0].stations)):\n",
    "                ''' use lat/lon paris only in and around NZ remove all others '''\n",
    "                if(st_inv[0].stations[each_st].latitude < 0 and st_inv[0].stations[each_st].longitude > 0):\n",
    "                    each_st_type_dict = st_inv[0].stations[each_st].get_contents()\n",
    "                    ''' get the second character representing the station type '''\n",
    "#                    st_type_dict[\"st_type\"].append(each_st_type_dict[\"channels\"][0][-3:-1])\n",
    "                    ''' list of corresponding station locations (lat / lon) '''\n",
    "                    st_list.append([st_inv[0].stations[each_st].code, each_st_type_dict[\"channels\"][0][-3:-1], st_inv[0][each_st].latitude, st_inv[0][each_st].longitude])\n",
    "                else:\n",
    "                    '''dictionary of all stations not in NZ visinity '''\n",
    "                    invalid_st_list.append([st_inv[0].stations[each_st].code,st_inv[0][each_st].latitude, st_inv[0][each_st].longitude])\n",
    "\n",
    "        except Exception as err:\n",
    "            print(\"Error message:\", err)\n",
    "        \n",
    "        return st_list, invalid_st_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of stataions by type\n",
    "\n",
    "Prepare an array of tuples necessary and sufficient station data:\n",
    "* _station code_ as a unique identifier\n",
    "* _coordinates_ longitude & latitude\n",
    "* _elevation_ in meters above mean sea level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    st_meta = station_data()\n",
    "#    st_loc_list = []\n",
    "#    st_inv_err_dict = []\n",
    "    st_list, invalid_st_list = st_meta.get_stations()\n",
    "    print('Invalid stations:',invalid_st_list)\n",
    "    print('Number of valid active stations:', len(st_list))\n",
    "#    unique_st_types = set(st_type_dict[\"st_type\"])\n",
    "    print('Unique station types:',set(item[1] for item in st_list))        \n",
    "    \n",
    "except Exception as err:\n",
    "    print(\"Error message:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define fault lines\n",
    "\n",
    "#### Class of Fault line methods\n",
    "\n",
    "We have completed objective 1.A. However, we will also include a mapping of the fault lines to give a perception of the station distribution relative to that of the map of fault lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fault_data():\n",
    "\n",
    "    ''' TODO at initiatlization download latest dataset from GeoNet then extract the *.json\n",
    "    '''\n",
    "    def _init_(self):\n",
    "        pass\n",
    "\n",
    "    ''' Extract nested values from a JSON tree to build a list of fault lines\n",
    "        containing the fault name and lat / lon pairs of the path\n",
    "    '''\n",
    "    \n",
    "    def get_paths(self):\n",
    "        import json\n",
    "        from dictor import dictor\n",
    "        \n",
    "        try:\n",
    "            with open('/home/nuwan/workspace/quasar/data/NZAFD/JSON/NZAFD_Oct_2020_WGS84.json') as json_file: \n",
    "                data = json.load(json_file)\n",
    "\n",
    "            faults = []\n",
    "            for each_feature in range(len(data['features'])):\n",
    "                flt = dictor(data,'features.{}.attributes.NAME'.format(each_feature))\n",
    "                name = 'Unnamed fault' if flt==\" \" else flt\n",
    "                points = []\n",
    "                path = dictor(data,'features.{}.geometry.paths.0'.format(each_feature))\n",
    "                for each_coordinate in range(len(path)):\n",
    "                    points.append([path[each_coordinate][0],path[each_coordinate][1]])\n",
    "                faults.append([name,points])\n",
    "\n",
    "        except Exception as err:\n",
    "            print(\"Error message:\", err)\n",
    "        return faults\n",
    "\n",
    "    '''\n",
    "        Interpolate more points for each fault line; if the distance between points > 1.5Km @ 0.5Km intervals\n",
    "        Otherwise, fit a single halfway point\n",
    "    '''\n",
    "    def interpolate_paths(self, paths, distance=float(2.5)):\n",
    "        from shapely.geometry import LineString\n",
    "        \n",
    "        interp_paths = []\n",
    "        try:\n",
    "            ''' loop through each fault path to breakdown into line segments; i.e. coordinate pairs '''\n",
    "            for path in range(len(paths)):\n",
    "                path_index = 0\n",
    "                ''' add the two line segment coordinates to begin with\n",
    "                    now loop through each path line segment to add interpolated points  '''\n",
    "                while (path_index < len(paths[path][1])-1):\n",
    "                    ip = []     # interpolated point\n",
    "                    rel_origin_coord = paths[path][1][path_index]     # relative starting point of the path\n",
    "                    rel_nn_coord = paths[path][1][path_index+1]\n",
    "\n",
    "                    ''' change to a while loop until all distances between consecutive points < delta_distance'''\n",
    "                    while LineString([rel_origin_coord, rel_nn_coord]).length*6371.0 > distance:\n",
    "                        ip = LineString([rel_origin_coord,rel_nn_coord]).interpolate((10.0**3)/6371.0, normalized=True).wkt\n",
    "                        # convertion needs to happen otherwise throws an exception\n",
    "                        ip_lat = float(ip[ip.find(\"(\")+1:ip.find(\")\")].split()[0])\n",
    "                        ip_lon = float(ip[ip.find(\"(\")+1:ip.find(\")\")].split()[1])\n",
    "                        rel_nn_coord = list([ip_lat,ip_lon])\n",
    "                        ''' If you want to add the already interpolated coordinates to the path to possibly speedup\n",
    "                        and use those points to create a denser path; note that it may will results in uniequal\n",
    "                        distant between consecutive points in the path. Comment the instruction below to disable.\n",
    "                        '''\n",
    "                        paths[path][1].insert(path_index+1,rel_nn_coord)    # interpolated coordinates closest to the relative origin\n",
    "\n",
    "                    path_index += 1\n",
    "\n",
    "                interp_paths.append([paths[path][0], paths[path][1]])\n",
    "\n",
    "        except Exception as err:\n",
    "            print(\"Error message:\", err)\n",
    "\n",
    "        return interp_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault line analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import LineString\n",
    "\n",
    "try:\n",
    "    faults = fault_data()     # declare fault lines class\n",
    "    original_paths = faults.get_paths()     # get all fault line paths\n",
    "\n",
    "    ''' analyse the distance between fault line path coordinates '''\n",
    "    print(\"Statistics of {} original fault lines before interpolating\".format(len(original_paths)))\n",
    "    for path in range(len(original_paths)):\n",
    "        sum_lengths = float(0)\n",
    "        for coords in range(len(original_paths[path][1])-1):\n",
    "            sum_lengths += LineString([original_paths[path][1][coords], \n",
    "                                       original_paths[path][1][coords+1]]).length*6371.0 \n",
    "        print(\"{0} has {1} coordinates with an average inter-coordinate distance: {2} Km\".format(original_paths[path][0], len(original_paths[path][1]), str(sum_lengths/len(original_paths[path][1]))))\n",
    "\n",
    "    interpolated_paths = faults.interpolate_paths(paths=original_paths,distance=2.5)\n",
    "    print(\"\\nwait until interpolation is complete ...\")\n",
    "    print(\"\\nPost interpolation statistics of {} fault lines\".format(len(interpolated_paths)))\n",
    "    for path in range(len(interpolated_paths)):\n",
    "        sum_lengths = float(0)\n",
    "        for coords in range(len(interpolated_paths[path][1])-1):\n",
    "            sum_lengths += LineString([interpolated_paths[path][1][coords], \n",
    "                                   interpolated_paths[path][1][coords+1]]).length*6371.0 \n",
    "        print(\"{0} has {1} coordinates with an average inter-coordinate distance: {2} Km\".format(interpolated_paths[path][0], len(interpolated_paths[path][1]), str(sum_lengths/len(interpolated_paths[path][1]))))\n",
    "\n",
    "except Exception as err:\n",
    "    print(\"Error message:\", err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJECTIVE 1.B - STATION CLUSTERS\n",
    "\n",
    "### Cluster Stations\n",
    "\n",
    "Apply DBSCAN to cluster stations with an epsilon < 30Km. DBSCAN is preferred over K-means clustering because K-means clustering considance the variance while DBSCAN considers a distance function. It gives the capacity to build clusters serving the criteria of < 30Km distance between stations.\n",
    "\n",
    "#### Data clensing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster\n",
    "1. Apply DBSCAN\n",
    "   1. Inherent __problem of DBSCAN__ is that it characterises data points to be in the same clusted if pair-wise data points satisfy the epsilon condition. This would not adequately satisfy the required condition that all data points in a a cluster are within the desired epsilon distance.\n",
    "1. Compute the cluster property measures to estimate the acceptability\n",
    "1. Dump the output to a file including cluster label, lat/lon, station code, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "import sklearn.utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "def get_dbscan_labels(st_arr):\n",
    "    err=\"0\"\n",
    "    try:\n",
    "        X, labels_true = make_blobs(n_samples=len(st_arr), centers=st_arr, cluster_std=0.4,random_state=0)\n",
    "        db = DBSCAN(eps=30.0/6371.0, min_samples=3, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "        print('DBSCAN epsilon:',db.eps,'algorithm:', db.algorithm, 'metric: ', db.metric)\n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "#        print('core samples mask', len(core_samples_mask),core_samples_mask)\n",
    "        labels = db.labels_\n",
    "#        print(\"DBSCAN found %0.3f labels\" % labels )\n",
    "    except Exception as err:\n",
    "        print(\"Error message:\", err)\n",
    "        labels = \"\"\n",
    "    return labels, labels_true, core_samples_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric analysis of the clusters\n",
    "\n",
    "A large propotion of the sensors could be clustered to have, at least, 03 senors in a cluster and that they are < 30Km distance from each other; which also is the basis for the ```eps = 30.0/6371.0``` (epsilon convereted to radians using the length of Earth's radius 6371Km). The estimated number of _noise points_ tell us the number of sensors that didn't belong to any cluster. The particular geodedic data cannot be clustered with KD-Trees or any Tree algorithm. However, chosing ```algorithm = \"ball_tree\"``` is recommended as it is well suited for geospatial data clustering. The ```metric = \"haversine\"``` is naturally required to calculate the distance between two geographical points. Finally, the ```st_dbscan_arr``` comprising latitude and longitude decimal data is converted to radians to be consistent with using the _haversine_ distance funcation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all columns except the lat / lon pairs\n",
    "tmp_st_coord = np.delete(st_coord, [0,3],axis=1).astype(np.float)\n",
    "# Remove bad coordinates specific to NZ\n",
    "tmp_clean_st_coord = [item for item in tmp_st_coord if item[0] < 0 and item[1] > 0]\n",
    "# Convert to float to avoid throwing a datatype error in the plot function\n",
    "station_coordinates = np.array(tmp_clean_st_coord).astype(np.float)\n",
    "#print('Station coordinates:\\n',' '.join([str(elem) for elem in tmp_nolabel_arr])) \n",
    "\n",
    "# Run dbscan and get the labels\n",
    "labels, labels_true, core_samples_mask = get_dbscan_labels(station_coordinates)\n",
    "#print('core samples mask', len(core_samples_mask),core_samples_mask)\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Total number of stations: %d' % len(labels))\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "# Plot a histogram of the station distribution\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(f\"Adjusted Mutual Information: %0.3f\" % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(station_coordinates, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results\n",
    "1. plot clusters with varied colors unique to each cluster\n",
    "1. plot fault lines to show closes sensor in cluster to the fault line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "plt.figure(figsize=(30, 40))\n",
    "#nz_map = Basemap(width=15000,height=15000,projection='merc',\n",
    "#            resolution='l',lat_0=-40,lon_0=176.)\n",
    "#nz_map.drawcoastlines()\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = station_coordinates[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    # uncomment to plot the noise\n",
    "    #xy = station_coordinates[class_member_mask & ~core_samples_mask]\n",
    "    #plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "    #         markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.legend(loc='upper left', fontsize=20)\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Nearest Neighbour Distance Statistics\n",
    "\n",
    "Compute the mean distance between nearest neigbours of a minimum 3 points\n",
    "* https://scikit-learn.org/stable/modules/neighbors.html\n",
    "* https://pysal.org/notebooks/explore/pointpats/distance_statistics.html#Mean-Nearest-Neighbor-Distance-Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Augment station array with cluster number\n",
    "# Start a new station coorinates and details tuple\n",
    "st_list = []\n",
    "i=0\n",
    "for i in range(len(labels)):\n",
    "    st_row = [tmp_arr[i,0],labels[i],tmp_arr[i,1],tmp_arr[i,2],tmp_arr[i,3]]\n",
    "    st_list.append(list(st_row))\n",
    "\n",
    "clusters = list({item[1] for item in st_list})\n",
    "\n",
    "for each_cluster in clusters:\n",
    "    cluster_list = list(st_list[j] for j in range(len(st_list)) if st_list[j][1] == each_cluster)\n",
    "    cluster_arr = np.delete(cluster_list, [0,1,4],axis=1).astype(np.float)\n",
    "    nbrs = NearestNeighbors(n_neighbors=3, algorithm='brute', metric='haversine').fit(cluster_arr)\n",
    "    distances, indices = nbrs.kneighbors(cluster_arr)\n",
    "    print(nbrs.kneighbors_graph(cluster_arr).toarray())\n",
    "    \n",
    "    each_cluster_clique = client.get_stations(latitude=-42.693,longitude=173.022,maxradius=30.0/6371.0, starttime = \"2016-11-13 11:05:00.000\",endtime = \"2016-11-14 11:00:00.000\")\n",
    "    print(each_cluster_clique)\n",
    "    _=inventory.plot(projection=\"local\")\n",
    "    \n",
    "    break\n",
    "\n",
    "sorted_rank = sorted(st_list, key=lambda i: (int(i[1])), reverse=True)\n",
    "#print('Code, Cluster, Latitude, Longitude, Elevation')\n",
    "#print(sorted_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of DBSCAN results\n",
    "It is evident from the cluster with large volume of data points are spread across the geography. Therefore, DBSCAN is shown to be innopriate for clustering stations to estimate whether they hold the property of being 30Km within each other.\n",
    "\n",
    "Next we "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESOURCES\n",
    "1. [Global data services and standards](http://www.fdsn.org/services/) offered by the International Federation Data of Seismic Networks (FDSN). \n",
    "1. GEONET resources:\n",
    "   1. [Stream Naming Conventions](https://www.geonet.org.nz/data/supplementary/channels) are based on historical usage together with recommendations from the [SEED manual](https://www.fdsn.org/seed_manual/SEEDManual_V2.4.pdf)\n",
    "   1. [Python tutorials](https://www.geonet.org.nz/data/tools/Tutorials) for using GeoNet resources\n",
    "1. [Seismo-Live](https://krischer.github.io/seismo_live_build/html/Workshops/2017_Baku_STCU_IRIS_ObsPy_course/07_Basic_Processing_Exercise_solution_wrapper.html) examples of get station waveform, inventory, event, arrival time, response, and plotting using obspy\n",
    "1. Choosing [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) over KMeans: \n",
    "   1. Fundermentally KMeans requires us to first select the number of clusters we wish to find and DBSCAN doesn't.\n",
    "   1. [clustering to reduce spatial data sizes](https://geoffboeing.com/2014/08/clustering-to-reduce-spatial-data-set-size/) KMeans is not an ideal algorithm for latitude-longitude spatial data because it minimizes variance, not geodetic distance. \n",
    "   1. [Explanation of DBSCAN clustering](https://towardsdatascience.com/explaining-dbscan-clustering-18eaf5c83b31) also identifies a drawback of KMeans clustering as it is vulnerable to outliers and outliers have a significant impact on the way the centroids moves.\n",
    "1. [Example of scikit-learn DBSCAN](https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html)\n",
    "1. [obspy.geodetics](https://docs.obspy.org/packages/obspy.geodetics.html) - various geodetic utilities for ObsPy - try an alternative clustering method with obspy geodetics\n",
    "1. Mapping tutorials\n",
    "   1. Visualization: [Mapping Global Earthquake Activity](http://introtopython.org/visualization_earthquakes.html)\n",
    "   1. Plotting data on a map [(Example Gallery)](https://matplotlib.org/basemap/users/examples.html)\n",
    "1. Calculating a [perpendicular distance to a line](https://math.stackexchange.com/questions/993236/calculating-a-perpendicular-distance-to-a-line-when-using-coordinates-latitude), when using coordinates (latitude & longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
