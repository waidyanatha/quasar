{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuaSaR: Identifying EEW Rings - MMI Classifier\n",
    "\n",
    "In the efforts to understand the GeoNet datasets for havesting data that can be used in trialing the picking agorithms, we begin with the [GeoNet Strong Motion Database](https://www.geonet.org.nz/data/supplementary/nzsmdb), [rupture  model data](), and [processed recordings]() that are readily available<sup>[1](#myftnote1)</sup>. The idea is to classify the historic data by the various measures made available throught the datasets. Some of the measures include the moment magnitude, hycenter location, measuring station locations, tectonic type, rupture length, total duration, and so on.\n",
    "\n",
    "<a name=\"ftnote1\">[1]</a>: [All GeoNet data and images](https://github.com/GeoNet/data), with updates on Github, are made available free of charge through the GeoNet project to facilitate research into hazards and assessment of risk. GeoNet is sponsored by the New Zealand Government through its agencies: Earthquake Commission (EQC), GNS Science and Land Information New Zealand (LINZ), the National Emergency Management Agency (NEMA) and the Ministry of Business, Innovation and Employment (MBIE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    WARNING CONTROL to display or ignore all warnings\n",
    "'''\n",
    "import warnings; warnings.simplefilter('default')     #switch betweeb 'default' and 'ignore'\n",
    "\n",
    "import logging\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hueristically mining the strong motion database\n",
    "\n",
    "[Flatfile specifications](https://static.geonet.org.nz/info/resources/applications_data/earthquake/strong_motion/Flatfiles_ColumnExplanation.pdf). They contain the horizontal and vertical acceleration response spectra, and horizontal and vertical Fourier amplitude spectra of acceleration.\n",
    "\n",
    "We begin with mining the data with attention \n",
    "1. _Mw_: Moment Magnitude\n",
    "1. _Orign_time_: Earthquake originating time\n",
    "1. _TectClass_: Tectonic Class of crustal, slab, or interface\n",
    "1. _Mech_: Focal mechanism whether it is a slip, strike, etc\n",
    "1. _Length_km_: rupture length in Kilometers\n",
    "1. _Width_km_: rupture width in Kilometers\n",
    "1. _TotalDuration_: Total duration of the earthquake\n",
    "\n",
    "### Class to load and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import torch\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "#from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "'''\n",
    "    CSV LOAD into DataFrame and remove unnecessary columns\n",
    "'''\n",
    "strong_motion_df = pd.read_csv('../data/flatfiles/NZdatabase_flatfile_Significant_Duration_horizontal.csv', encoding = \"UTF-8\")\n",
    "\n",
    "strong_motion_df = strong_motion_df.drop(columns=['CuspID','References','Location','Record'], axis=1)\n",
    "strong_motion_df = strong_motion_df.replace('>2',float(2.1))\n",
    "strong_motion_df = strong_motion_df.replace('<0.1',float(0.1))\n",
    "''' Convert datetime to a defined as YYYYMMDD time rounded to 0 or 1 day '''\n",
    "for t_idx, t_val in enumerate(strong_motion_df['Origin_time']):\n",
    "    t_dt = dt.datetime.fromisoformat(t_val[0:19])\n",
    "    t_float = float(t_dt.year*100000+t_dt.month*1000+t_dt.day*10+(round(t_dt.hour/24)))\n",
    "    strong_motion_df['Origin_time'][t_idx]=t_float\n",
    "\n",
    "_lst_cate_data_cols = [\n",
    "                        'Origin_time',     # Origin time of earthquake in UTC\n",
    "                        'TectClass',       # Tectonic classification, either ‘crustal’, ‘interface’, or ‘slab’\n",
    "                        'Mech',            #\n",
    "                        'HWFW',            #\n",
    "                        'SiteCode',        #\n",
    "                        'SiteClass1170',   #\n",
    "                        'Vs30Uncert',      #\n",
    "                        'TsiteUncert',     #\n",
    "                        'Z1Uncertainty'    #\n",
    "                      ]\n",
    "cat_strong_motion_df = strong_motion_df[_lst_cate_data_cols]\n",
    "num_strong_motion_df = strong_motion_df.drop(_lst_cate_data_cols, axis=1)\n",
    "print(f\"Shape of the Categorical DataFrame: {cat_strong_motion_df.shape}\")\n",
    "print(f\"Shape of the Numerical DataFrame: {num_strong_motion_df.shape}\")\n",
    "\n",
    "''' LabelEncoder to convert the categorical data to numerical float64 '''\n",
    "le = LabelEncoder()\n",
    "for __cat_col_name in _lst_cate_data_cols:\n",
    "    strong_motion_df[__cat_col_name] = le.fit_transform(strong_motion_df[__cat_col_name]).astype(float) \n",
    "print(f\"Shape of the full DataFrame: {strong_motion_df.shape}\")\n",
    "print(f'\\nPost label encoding of categorical data \\n{strong_motion_df.head(3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to pariwise plot measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "'''\n",
    "    Pairwise density plots for all the column variables\n",
    "    attribute specs: https://static.geonet.org.nz/info/resources/applications_data/earthquake/strong_motion/Flatfiles_ColumnExplanation.pdf\n",
    "'''\n",
    "plot_df = strong_motion_df[[\n",
    "                            'Mw',          # Moment Magnitude\n",
    "                            'MwUncert',    # Mw uncertainty class \n",
    "                            'Origin_time', # Origin time of earthquake in UTC\n",
    "                            'TectClass',   # Tectonic classification, either ‘crustal’, ‘interface’, or ‘slab’\n",
    "                            'Mech',        # Focal mechanism: S→strike-slip, N→normal,R→reverse,U→unknown\n",
    "                            'PreferredFaultPlane', # 1→one fault plane orientation is preferred, 1→Unknown\n",
    "                            'Strike',      # Strike angle (degrees)\n",
    "                            'Dip',         # Dip angle (degrees)\n",
    "                            'Rake',        # Rake angle (degrees)\n",
    "                            'HypLat',      # Hypercenter Latitude\n",
    "                            'HypLon',      # Hypercernter Longitude\n",
    "                            'StationLat',  # Recording Station Latitude\n",
    "                            'StationLon',  # Recording Station Longitude\n",
    "                            'HypN',        # Northing of Hyppercenter\n",
    "                            'HypE',        # Easting of Hypercernter\n",
    "                            'StationN',    # Northing of Station\n",
    "                            'StationE',    # Easting of Station\n",
    "                            'LENGTH_km',   # Infered rupture Length in Kilometers\n",
    "                            'WIDTH_km',    # Infered down-dip rupture Width in Kilometers\n",
    "                            'TotalDuration'# Total Duration of the earthquake\n",
    "                           ]]\n",
    "lst_plot_cols = ['Origin_time','Mw','TectClass','Mech','LENGTH_km','TotalDuration']\n",
    "print(f\"Description of each measure: \\n{pd.DataFrame(plot_df[lst_plot_cols].describe(include='all')).T}\")\n",
    "g = sns.pairplot(plot_df[lst_plot_cols], \n",
    "             hue='Mw', corner=True,hue_order=None,\n",
    "             kind='scatter', diag_kind='auto', height=7,markers='d')\n",
    "g.fig.suptitle(\"Pair Plots for relevant measures\") # y= some height>1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying an NN Classifier\n",
    "The intent is to use data available from GeoNet strong motion and felt databases to encapsulate a a Modified Mercalli Intensity (MMI). We use an Artifical Nueral Network (NN) to classify the data from the several flat files. The reason to use an NN is because of the large volume of covariates in the dataset. Therefore, the output should deliver an MMI for a new scenario.\n",
    "\n",
    "In this notebook we are investigating the use of pytorch and tensor products and their capabilties to build a model for evaluating Objective II.B. For such we need to achieve the following steps\n",
    "1. Encode or vectorize the data; especially with transforming categorical labels to numerical data\n",
    "1. [Split the data](https://palikar.github.io/posts/pytorch_datasplit/) to generate training, test, and validation datasets\n",
    "1. Transform them into tensors as inputs for the model\n",
    "\n",
    "### Method to split dataset into train, validation, & test\n",
    "We borrow from [Train-Validation-Test split in PyTorch](https://palikar.github.io/posts/pytorch_datasplit/#the-datasplit-class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' METHOD DATALOADER - build the training dataloader object'''\n",
    "# Load necessary Pytorch packages\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "\n",
    "shuffle = True\n",
    "test_train_split = 0.8\n",
    "val_train_split = 0.2\n",
    "\n",
    "dataset = strong_motion_df.drop(['Origin_time','SiteCode'], axis=1).astype(float)\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "test_split = int(np.floor(test_train_split * dataset_size))\n",
    "\n",
    "if shuffle:\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, test_indices = indices[:test_split], indices[test_split:]\n",
    "train_size = len(train_indices)\n",
    "validation_split = int(np.floor((1 - val_train_split) * train_size))\n",
    "train_indices, val_indices = train_indices[ : validation_split], train_indices[validation_split:]\n",
    "\n",
    "#_targets = dataset[['TectClass','Mech']]\n",
    "#_inputs = dataset.drop(['TectClass','Mech'], axis=1)\n",
    "_targets = dataset['Mech']\n",
    "_inputs = dataset.drop(['Mech'], axis=1)\n",
    "print('Classes',set(_targets))\n",
    "train_inputs = np.array(_inputs.iloc[train_indices], dtype=np.float32)\n",
    "train_targets= np.array(_targets.iloc[train_indices], dtype=np.float32)\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(train_inputs)):\n",
    "    train_data.append([train_inputs[i], train_targets[i]])\n",
    "#train_data_ts = torch.tensor([train_data], dtype=torch.float32)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=10, num_workers=0, shuffle=False)\n",
    "print(\"Train data loader length\", str(len(train_loader)))\n",
    "\n",
    "'''\n",
    "for data in train_data:\n",
    "    print(data)\n",
    "    break\n",
    "for i, (data,labels) in enumerate(train_data, 1):\n",
    "    print(data,type(data))\n",
    "    print(labels,type(labels))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NN Model\n",
    "\n",
    "#### Input layer\n",
    "Comprises both numerical and categorical data. We have generated embeddings for all the categorical data.\n",
    "* ```n_features: int=46``` (of 55)\n",
    "\n",
    "#### Hidden Layer\n",
    "The design of the hidden layer makes use of\n",
    "1. [Linear transformation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) takes ```n_features: int=46``` (of 55) and ```n_out_feature: int=2``` (of 5).\n",
    "\n",
    "#### Output Layer\n",
    "The output classifiers are ```output={'Mw','TectClass','Mech','Length_km','Width_km', 'TotalDuration'}```. Therefore, we have ```n_out_features: int=6```.\n",
    "\n",
    "#### Weights\n",
    "The weights are randomly generated for the ```2401 x 49``` tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_in_features: int, n_out_features: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_in_features, n_out_features)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Pass data through conv1\n",
    "#        x = self.conv1(x)\n",
    "        # Use the rectified-linear activation function over x\n",
    "#        x = F.relu(x)\n",
    "\n",
    "#        x = self.conv2(x)\n",
    "#        x = F.relu(x)\n",
    "\n",
    "        # Run max pooling over x\n",
    "#        x = F.max_pool2d(x, 2)\n",
    "        # Pass data through dropout1\n",
    "#        x = self.dropout1(x)\n",
    "        # Flatten x with start_dim=1\n",
    "#        x = torch.flatten(x, 1)\n",
    "        # Pass data through fc1\n",
    "        x = self.fc1(x)\n",
    "#        print(x)\n",
    "#        x = F.relu(x)\n",
    "#        x = self.dropout2(x)\n",
    "#        x = self.fc2(x)\n",
    "\n",
    "        # Apply softmax to x\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to initialize the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "train_loader_input, train_loader_target = next(iter(train_loader))\n",
    "print(f\"Train loader shape {train_loader_input.shape} and target loader shape {train_loader_target.shape}\")\n",
    "n_in_features = train_loader_input.shape[1]     # = 51\n",
    "n_out_features = len(set(_targets))\n",
    "#n_out_features = train_loader_target.shape[0]   # = 2\n",
    "print(f'Building model with {n_in_features} in features and {n_out_features} out features')\n",
    "model = Net(n_in_features,n_out_features) # On CPU\n",
    "print(f\"\\n{model}\\nWeights: {model.fc1.weight.shape}\\nBias: {model.fc1.bias.shape}\")\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 300\n",
    "aggregated_losses = []\n",
    "\n",
    "#categorical_train_data = train_df[_lst_cate_data_cols]\n",
    "#numerical_train_data = \n",
    "\n",
    "print('Input train tensor data type:',train_loader_input.dtype,  'and', train_loader_input.size())\n",
    "print('Target train tensor data type:',train_loader_target.dtype,  'and', train_loader_target.size())\n",
    "\n",
    "print(f\"Begin training for {epochs} epochs ...\\n\")\n",
    "for i in range(epochs):\n",
    "    i += 1\n",
    "    for k, (data, labels) in enumerate(train_loader):\n",
    "        y_pred = model.forward(data)\n",
    "#        print(f'{k} Output shape {y_pred.shape} and tensor looks like\\n{y_pred[0:2]}')\n",
    "        labels = labels.type(torch.LongTensor)\n",
    "#        print(f'{k} Label shape {labels.shape} and tensor looks like\\n{labels}')\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        aggregated_losses.append(single_loss)\n",
    "\n",
    "    if i%25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    single_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_lst_outputs = ['Origin_time','Mw','TectClass','Mech','LENGTH_km','TotalDuration']\n",
    "outputs = torch.tensor(tmp_sm_df[_lst_outputs].values)\n",
    "print(f\"{outputs.shape} \\n{outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoding [DEPRECATE?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    DEPRECATE if the LabelEncoding is sufficient; else,\n",
    "    TODO: OneHotEncode before building the the tensors\n",
    "'''\n",
    "\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "\n",
    "logging.debug('Preprocessing data with OneHotEncoder')\n",
    "\n",
    "'''\n",
    "    OneHotEncoder to create the arrays for training, validation, and testing\n",
    "'''\n",
    "train_1hotenc = np.empty_like(train_ts)\n",
    "for t_indx, t in enumerate(train_ts):\n",
    "    print(t_indx,t.numpy())\n",
    "    train_1hotenc[t_indx] = OneHotEncoder(categories='auto', drop=None, sparse=True, dtype='float64', \n",
    "                                  handle_unknown='error').fit_transform(t.numpy())\n",
    "    print(train_1hotenc[t_indx])\n",
    "train_1hotenc_ts = train_1hotenc\n",
    "print(f\"Shape of the OneHotencoded array: {train_1hotenc.shape}\")\n",
    "print(f\"Datatype of the OneHotencoded array: {train_1hotenc.dtype}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
