{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuaSaR: Identifying EEW Rings - Topology\n",
    "\n",
    "[Quake Safe Rings](./1a_stations_faultlnes_plot.ipynb) - in our efforts to understand the station fault topology - we make use of the International Federation Data of Seismic Networks (FDSN), the global standard and a [data service](http://www.fdsn.org/services/) for sharing seismic sensor wave form data. The Obspy librarires support FDSN. The list of resources and services that are used for retrieving station inventory and waveform data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from obspy import read_inventory\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.core import read, UTCDateTime\n",
    "#from datetime import date\n",
    "\n",
    "# Establish start and end time for retrieving waveform data\n",
    "t_start = UTCDateTime.now()-518400 #6 days ago = 60s x 60m x 24h x 6d\n",
    "t_end = UTCDateTime.now()+86400 #1 day in the future = 60s x 60m x 24h\n",
    "print('Station startime: ', t_start, '\\n & ending time: ', t_end)\n",
    "\n",
    "try:\n",
    "    #use either or GeoNet station service webservice URL or Obspy FDSN Client protocol to retrieve station data\n",
    "    st_ws = 'https://service.geonet.org.nz/fdsnws/station/1/query?network=NZ&level=station&endafter=2020-12-31&format=xml'\n",
    "    #st_ws = 'https://service.geonet.org.nz/fdsnws/station/1/query?network=NZ&station=CECS&level=channel'\n",
    "    # Set FDSN client URL to GEONET short code\n",
    "    client  = Client('GEONET')\n",
    "    print(\"Client is\",client)\n",
    "except Exception as err:\n",
    "    print(\"Error message:\", err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all Station details\n",
    "\n",
    "An initiatl step for object 1.A is determining the the types of operational seismic sensors and their locations. GoeNet hosts wave forms for a multitude of [sensor types](https://api.geonet.org.nz/network/sensor/type) (e.g. tidle guages, pressure gauges, seismometers, GNSS antennas, barometers, Microphones, Hydrophones and so on). The focus is on motion sensors of type: (i) accelerometer, (ii) broadband velocity, (iii) short period velocity, and (iv) GNSS. Furthermore, the sensors location code is unique to each sensor type. Therefore, one may chose to use the location code prefix or sensor type enumerator to select the desired sensors; i.e. seimograph and accelerometer stations. The motion sensors are used in both earthquake and volcanic seismic activity monitoring and early warning.\n",
    "\n",
    "_Sensor types that are relevant to earthquake detection are:_\n",
    "* 1 Accelerometer \n",
    "* 3 Broadband Seismometer \n",
    "* 4 GNSS Antenna \n",
    "* 8 Short Period Borehole Seismometer \n",
    "* 9 Short Period Seismometer \n",
    "* 10 Strong Motion Sensor\n",
    "\n",
    "_Location codes reserved for the seismic sensors are:_\n",
    "* 1? - weak motion sensors\n",
    "* 2? - strong motion sensors\n",
    "\n",
    "_Channel codes are:_ \n",
    "\n",
    "Defined in the GeoNet's [stream naming conventions](https://www.geonet.org.nz/data/supplementary/channels)\n",
    "First letter of the code represents a combination of sampling rate and sensor bandwidth\n",
    "\n",
    "First letter represts the sensor type \n",
    "* U (Ultra Long Period sampled at 0.01Hz, or SOH sampled at 0.01Hz)\n",
    "* V (Very Long Period sampled at 0.1Hz, or SOH sampled at 0.1Hz)\n",
    "* L (Broad band sampled at 1Hz, or SOH sampled at 1Hz)\n",
    "* B (Broad band sampled at between 10 and 80 Hz, usually 10 or 50 Hz)\n",
    "* S (Short-period sampled at between 10 and 80 Hz, usually 50 Hz)\n",
    "* H (High Broad band sampled at or above 80Hz, generally 100 or 200 Hz)\n",
    "* E (Extremely Short-period sampled at or above 80Hz, generally 100 Hz)\n",
    "\n",
    "The second letter represents the sensor type, e.g.(listed are the ones relevant to seismometers)\n",
    "* H (Weak motion sensor, e.g. measuring velocity)\n",
    "* N (Strong motion sensor, e.g. measuring acceleration)\n",
    "* L (Low gain sensor, usually velocity)\n",
    "* M (Mass position, used for monitoring broadband sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' All weak & strong motion, low gain, and mass possion sensor types '''\n",
    "channels = \"UH*,VH*,LH*,BH*,SH*,HH*,EH*,UN*,VN*,LN*,BN*,SN*,HN*,EN*\"\n",
    "st_type = {\"UH\" : \"Weak motion sensor, e.g. measuring velocity\\nUltra Long Period sampled at 0.01Hz, or SOH sampled at 0.01Hz\",\n",
    "           \"VH\" : \"Weak motion sensor, e.g. measuring velocity\\nVery Long Period sampled at 0.1Hz, or SOH sampled at 0.1Hz\",\n",
    "           \"LH\" : \"Weak motion sensor, e.g. measuring velocity\\nBroad band sampled at 1Hz, or SOH sampled at 1Hz\",\n",
    "           \"BH\" : \"Weak motion sensor, e.g. measuring velocity\\nBroad band sampled at between 10 and 80 Hz, usually 10 or 50 Hz\",\n",
    "           \"SH\" : \"Weak motion sensor, e.g. measuring velocity\\nShort-period sampled at between 10 and 80 Hz, usually 50 Hz\", \n",
    "           \"HH\" : \"Weak motion sensor, e.g. measuring velocity\\nHigh Broad band sampled at or above 80Hz, generally 100 or 200 Hz\",\n",
    "           \"EH\" : \"Weak motion sensor, e.g. measuring velocity\\nExtremely Short-period sampled at or above 80Hz, generally 100 Hz\",\n",
    "           \"UN\" : \"Strong motion sensor, e.g. measuring acceleration\\nUltra Long Period sampled at 0.01Hz, or SOH sampled at 0.01Hz\",\n",
    "           \"VN\" : \"Strong motion sensor, e.g. measuring acceleration\\nVery Long Period sampled at 0.1Hz, or SOH sampled at 0.1Hz\",\n",
    "           \"LN\" : \"Strong motion sensor, e.g. measuring acceleration\\nBroad band sampled at 1Hz, or SOH sampled at 1Hz\",\n",
    "           \"BN\" : \"Strong motion sensor, e.g. measuring acceleration\\nBroad band sampled at between 10 and 80 Hz, usually 10 or 50 Hz\",\n",
    "           \"SN\" : \"Strong motion sensor, e.g. measuring acceleration\\nShort-period sampled at between 10 and 80 Hz, usually 50 Hz\",\n",
    "           \"HN\" : \"Strong motion sensor, e.g. measuring acceleration\\nHigh Broad band sampled at or above 80Hz, generally 100 or 200 Hz\",\n",
    "           \"EN\" : \"Strong motion sensor, e.g. measuring acceleration\\nExtremely Short-period sampled at or above 80Hz, generally 100 Hz\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of stataions\n",
    "\n",
    "Prepare an array of tuples necessary and sufficient station data:\n",
    "* _station code_ as a unique identifier\n",
    "* _coordinates_ longitude & latitude\n",
    "* _elevation_ in meters above mean sea level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "#plt.figure(figsize=(30, 40))\n",
    "#fig, ax = plt.subplots(1,1,figsize=(30,40))\n",
    "\n",
    "try:\n",
    "    st_inv = client.get_stations(network='NZ', location=\"1?,2?\", station='*', \n",
    "                             channel=channels, level='channel', starttime=t_start, endtime = t_end)\n",
    "\n",
    "    ''' Print the station inventory '''\n",
    "    print('Number of active stations:', len(st_inv[0].stations))\n",
    "    st_type_dict = {\"st_type\":[]}\n",
    "    st_loc_list = []\n",
    "    st_inv_err_dict = []\n",
    "\n",
    "    for each_st in range(len(st_inv[0].stations)):\n",
    "        ''' use lat/lon paris only in and around NZ'''\n",
    "        if(st_inv[0].stations[each_st].latitude < 0 and st_inv[0].stations[each_st].longitude > 0):\n",
    "            each_st_type_dict = st_inv[0].stations[each_st].get_contents()\n",
    "            ''' get the second character representing the station type '''\n",
    "            st_type_dict[\"st_type\"].append(each_st_type_dict[\"channels\"][0][-3:-1])\n",
    "            ''' list of corresponding station locations (lat / lon) '''\n",
    "            st_loc_list.append([st_inv[0].stations[each_st].code, each_st_type_dict[\"channels\"][0][-3:-1], st_inv[0][each_st].latitude, st_inv[0][each_st].longitude])\n",
    "        else:\n",
    "            st_inv_err_dict.append([st_inv[0].stations[each_st].code,st_inv[0][each_st].latitude, st_inv[0][each_st].longitude])\n",
    "\n",
    "    print('Invalid stations:',st_inv_err_dict)\n",
    "    print('Number of valid active stations:', len(st_loc_list))\n",
    "\n",
    "    unique_st_types = set(st_type_dict[\"st_type\"])\n",
    "    '''\n",
    "    core_samples_mask = np.zeros_like(st_type_dict[\"st_type\"], dtype=bool)\n",
    "    core_samples_mask[0:sum([len(x) for x in st_type_dict[\"st_type\"]])] = True\n",
    "\n",
    "    colors = [plt.cm.Spectral(each)\n",
    "              for each in np.linspace(0, 1, len(unique_st_types))]\n",
    "    '''\n",
    "    # convert list to array for color coding the locations by station types\n",
    "    '''\n",
    "    legend_elements = []\n",
    "    st_loc_arr = np.array(st_loc_list)\n",
    "    for k, col in zip(unique_st_types, colors):\n",
    "        class_member_mask = (np.array(st_type_dict[\"st_type\"]) == k)\n",
    "        xy = st_loc_arr[class_member_mask & core_samples_mask]\n",
    "        print('Number of',str(st_type[k]),': %d' % len(xy))\n",
    "\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "                 markeredgecolor='k', markersize=14, label=str(st_type[k]))\n",
    "    \n",
    "    plt.title('Estimated number of Stations: %d' % len(st_loc_arr), fontsize=40)\n",
    "    plt.xlabel('Latitude', fontsize=30)\n",
    "    plt.ylabel('Longitude', fontsize=30)\n",
    "    plt.xticks(size=20)\n",
    "    plt.yticks(size=20)\n",
    "    plt.legend(loc='upper left', fontsize=20)\n",
    "    plt.show()\n",
    "    '''\n",
    "\n",
    "except Exception as err:\n",
    "    print(\"Error message:\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import numpy as np\n",
    "\n",
    "st_coord = []\n",
    "\n",
    "try:\n",
    "    i = 0\n",
    "    for i in range(len(st_inv[0].stations)):\n",
    "        st_tuple = (st_inv[0][i].code, st_inv[0][i].latitude,st_inv[0][i].longitude, st_inv[0][i].elevation)\n",
    "        st_coord.append(tuple(st_tuple))\n",
    "        #st_coord = {'Code': st_inv[0][i].code,'Coordinates': {'Latitude': st_inv[0][i].latitude,'Longitude': st_inv[0][i].longitude},'Elavation': st_inv[0][i].elevation}\n",
    "#        print(st_coord[i])\n",
    "\n",
    "# Mapping stations\n",
    "except Exception as err:\n",
    "    print(\"Error message:\", err)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault line coordinates\n",
    "\n",
    "We have completed objective 1.A. However, we will also include a mapping of the fault lines to give a perception of the station distribution relative to that of the map of fault lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import json\n",
    "from dictor import dictor\n",
    "\n",
    "#plt.figure(figsize=(30, 40))\n",
    "\n",
    "'''Extract nested values from a JSON tree.'''\n",
    "\n",
    "try:\n",
    "    with open('/home/nuwan/workspace/quasar/data/NZAFD/JSON/NZAFD_Oct_2020_WGS84.json') as json_file: \n",
    "        data = json.load(json_file)\n",
    "\n",
    "    for each_feature in range(len(data['features'])):\n",
    "        points = {\"x\":[], \"y\":[]}\n",
    "        path = dictor(data,'features.{}.geometry.paths.0'.format(each_feature))\n",
    "        for each_coordinate in range(len(path)):\n",
    "            points[\"x\"].append(path[each_coordinate][0])\n",
    "            points[\"y\"].append(path[each_coordinate][1])\n",
    "\n",
    "except Exception as err:\n",
    "    print(\"Error message:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJECTIVE 1.A - STATION CLUSTERS\n",
    "\n",
    "### Cluster Stations\n",
    "\n",
    "Apply DBSCAN to cluster stations with an epsilon < 30Km. DBSCAN is preferred over K-means clustering because K-means clustering considance the variance while DBSCAN considers a distance function. It gives the capacity to build clusters serving the criteria of < 30Km distance between stations.\n",
    "\n",
    "#### Data clensing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster\n",
    "1. Apply DBSCAN\n",
    "   1. Inherent __problem of DBSCAN__ is that it characterises data points to be in the same clusted if pair-wise data points satisfy the epsilon condition. This would not adequately satisfy the required condition that all data points in a a cluster are within the desired epsilon distance.\n",
    "1. Compute the cluster property measures to estimate the acceptability\n",
    "1. Dump the output to a file including cluster label, lat/lon, station code, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "import sklearn.utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "def get_dbscan_labels(st_arr):\n",
    "    err=\"0\"\n",
    "    try:\n",
    "        X, labels_true = make_blobs(n_samples=len(st_arr), centers=st_arr, cluster_std=0.4,random_state=0)\n",
    "        db = DBSCAN(eps=30.0/6371.0, min_samples=3, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "        print('DBSCAN epsilon:',db.eps,'algorithm:', db.algorithm, 'metric: ', db.metric)\n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "#        print('core samples mask', len(core_samples_mask),core_samples_mask)\n",
    "        labels = db.labels_\n",
    "#        print(\"DBSCAN found %0.3f labels\" % labels )\n",
    "    except Exception as err:\n",
    "        print(\"Error message:\", err)\n",
    "        labels = \"\"\n",
    "    return labels, labels_true, core_samples_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric analysis of the clusters\n",
    "\n",
    "A large propotion of the sensors could be clustered to have, at least, 03 senors in a cluster and that they are < 30Km distance from each other; which also is the basis for the ```eps = 30.0/6371.0``` (epsilon convereted to radians using the length of Earth's radius 6371Km). The estimated number of _noise points_ tell us the number of sensors that didn't belong to any cluster. The particular geodedic data cannot be clustered with KD-Trees or any Tree algorithm. However, chosing ```algorithm = \"ball_tree\"``` is recommended as it is well suited for geospatial data clustering. The ```metric = \"haversine\"``` is naturally required to calculate the distance between two geographical points. Finally, the ```st_dbscan_arr``` comprising latitude and longitude decimal data is converted to radians to be consistent with using the _haversine_ distance funcation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all columns except the lat / lon pairs\n",
    "tmp_st_coord = np.delete(st_coord, [0,3],axis=1).astype(np.float)\n",
    "# Remove bad coordinates specific to NZ\n",
    "tmp_clean_st_coord = [item for item in tmp_st_coord if item[0] < 0 and item[1] > 0]\n",
    "# Convert to float to avoid throwing a datatype error in the plot function\n",
    "station_coordinates = np.array(tmp_clean_st_coord).astype(np.float)\n",
    "#print('Station coordinates:\\n',' '.join([str(elem) for elem in tmp_nolabel_arr])) \n",
    "\n",
    "# Run dbscan and get the labels\n",
    "labels, labels_true, core_samples_mask = get_dbscan_labels(station_coordinates)\n",
    "#print('core samples mask', len(core_samples_mask),core_samples_mask)\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Total number of stations: %d' % len(labels))\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "# Plot a histogram of the station distribution\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(f\"Adjusted Mutual Information: %0.3f\" % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(station_coordinates, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results\n",
    "1. plot clusters with varied colors unique to each cluster\n",
    "1. plot fault lines to show closes sensor in cluster to the fault line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "plt.figure(figsize=(30, 40))\n",
    "#nz_map = Basemap(width=15000,height=15000,projection='merc',\n",
    "#            resolution='l',lat_0=-40,lon_0=176.)\n",
    "#nz_map.drawcoastlines()\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = station_coordinates[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    # uncomment to plot the noise\n",
    "    #xy = station_coordinates[class_member_mask & ~core_samples_mask]\n",
    "    #plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "    #         markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.legend(loc='upper left', fontsize=20)\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Nearest Neighbour Distance Statistics\n",
    "\n",
    "Compute the mean distance between nearest neigbours of a minimum 3 points\n",
    "* https://scikit-learn.org/stable/modules/neighbors.html\n",
    "* https://pysal.org/notebooks/explore/pointpats/distance_statistics.html#Mean-Nearest-Neighbor-Distance-Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Augment station array with cluster number\n",
    "# Start a new station coorinates and details tuple\n",
    "st_list = []\n",
    "i=0\n",
    "for i in range(len(labels)):\n",
    "    st_row = [tmp_arr[i,0],labels[i],tmp_arr[i,1],tmp_arr[i,2],tmp_arr[i,3]]\n",
    "    st_list.append(list(st_row))\n",
    "\n",
    "clusters = list({item[1] for item in st_list})\n",
    "\n",
    "for each_cluster in clusters:\n",
    "    cluster_list = list(st_list[j] for j in range(len(st_list)) if st_list[j][1] == each_cluster)\n",
    "    cluster_arr = np.delete(cluster_list, [0,1,4],axis=1).astype(np.float)\n",
    "    nbrs = NearestNeighbors(n_neighbors=3, algorithm='brute', metric='haversine').fit(cluster_arr)\n",
    "    distances, indices = nbrs.kneighbors(cluster_arr)\n",
    "    print(nbrs.kneighbors_graph(cluster_arr).toarray())\n",
    "    \n",
    "    each_cluster_clique = client.get_stations(latitude=-42.693,longitude=173.022,maxradius=30.0/6371.0, starttime = \"2016-11-13 11:05:00.000\",endtime = \"2016-11-14 11:00:00.000\")\n",
    "    print(each_cluster_clique)\n",
    "    _=inventory.plot(projection=\"local\")\n",
    "    \n",
    "    break\n",
    "\n",
    "sorted_rank = sorted(st_list, key=lambda i: (int(i[1])), reverse=True)\n",
    "#print('Code, Cluster, Latitude, Longitude, Elevation')\n",
    "#print(sorted_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of DBSCAN results\n",
    "It is evident from the cluster with large volume of data points are spread across the geography. Therefore, DBSCAN is shown to be innopriate for clustering stations to estimate whether they hold the property of being 30Km within each other.\n",
    "\n",
    "Next we "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESOURCES\n",
    "1. [Global data services and standards](http://www.fdsn.org/services/) offered by the International Federation Data of Seismic Networks (FDSN). \n",
    "1. GEONET resources:\n",
    "   1. [Stream Naming Conventions](https://www.geonet.org.nz/data/supplementary/channels) are based on historical usage together with recommendations from the [SEED manual](https://www.fdsn.org/seed_manual/SEEDManual_V2.4.pdf)\n",
    "   1. [Python tutorials](https://www.geonet.org.nz/data/tools/Tutorials) for using GeoNet resources\n",
    "1. [Seismo-Live](https://krischer.github.io/seismo_live_build/html/Workshops/2017_Baku_STCU_IRIS_ObsPy_course/07_Basic_Processing_Exercise_solution_wrapper.html) examples of get station waveform, inventory, event, arrival time, response, and plotting using obspy\n",
    "1. Choosing [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) over KMeans: \n",
    "   1. Fundermentally KMeans requires us to first select the number of clusters we wish to find and DBSCAN doesn't.\n",
    "   1. [clustering to reduce spatial data sizes](https://geoffboeing.com/2014/08/clustering-to-reduce-spatial-data-set-size/) KMeans is not an ideal algorithm for latitude-longitude spatial data because it minimizes variance, not geodetic distance. \n",
    "   1. [Explanation of DBSCAN clustering](https://towardsdatascience.com/explaining-dbscan-clustering-18eaf5c83b31) also identifies a drawback of KMeans clustering as it is vulnerable to outliers and outliers have a significant impact on the way the centroids moves.\n",
    "1. [Example of scikit-learn DBSCAN](https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html)\n",
    "1. [obspy.geodetics](https://docs.obspy.org/packages/obspy.geodetics.html) - various geodetic utilities for ObsPy - try an alternative clustering method with obspy geodetics\n",
    "1. Mapping tutorials\n",
    "   1. Visualization: [Mapping Global Earthquake Activity](http://introtopython.org/visualization_earthquakes.html)\n",
    "   1. Plotting data on a map [(Example Gallery)](https://matplotlib.org/basemap/users/examples.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
