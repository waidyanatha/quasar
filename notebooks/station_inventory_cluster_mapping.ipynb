{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QuaSaR: Identifying EEW Rings\n",
    "\n",
    "## GOAL AND OBJECTIVES\n",
    "Quake Shielded Rings (QuSaR) are essentially self relient [Rings](https://brilliant.org/wiki/ring-theory/) of sensors sharing discretized time-series of wave form information to identify threats and forewarn to give man and machine a lead time to respond to harmful earthquakes.\n",
    "\n",
    "The overall __gaol__ is to examine how the GeoNet seismic network can be augmented with a low-cost network to offer low-latency EEWs by making use of cutting-edge earthquake picking algorithms and machine learning techniques. The expected outcome is for the findings to serve as evidence for supporting a strategic deployment of a ring or rings of micro-array networks. \n",
    "\n",
    "The intent is to also make use of the analysis and tools is to serve as inputs for earthquake hazard risk assessment. Thereby, a community interested in operationalizing their own micro-array ring can mae us of the analysis and tools to determining whether or not and how they may need to invest in building a micro-array ring.\n",
    "\n",
    "### Objectives\n",
    "1. _Understand the network dynamics (structure of connection of the units and their capabilities)_\n",
    "   1. Retrieve data on all the operational NZ seismic stations to __map the inventory__ by types and location.\n",
    "   1. Build a __nearest neigbour map of clusters__ of all the operational stations within a 30Km radius\n",
    "   1. Determine station cluster __topography__ relative to the __fault lines__ and earthquake detection role and capacity  \n",
    "1. _Apply earthquake __picking algorithms__ on the GeoNet wave form data_\n",
    "   1. Test the __standard GeoNet algorithms__ (e.g. LTS/STS, Pd, )\n",
    "   1. Test with new __machine learning and wavefield algorithms__ (e.g. , PLUM)\n",
    "   1. Test above picking algorithms with __simulated earthquakes__ and for __selected high risk faults__ to observe the response of the picking algorithms\n",
    "   \n",
    "1. _Determine ways for improving the station rings for an incremental effectiveness of EEW_\n",
    "   1. Propose to __fit additional stations__ to improve the 30Km nearest neighbour cluster; then show how that improves the picking\n",
    "   1. Apply the geodedic methodology to __interpolate seismic data__ for the proposed station locations \n",
    "   1. Try the earthquake __picking algorithms__ on the hypothetical network to measure effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJECTIVE 1.A - SENSOR MAP\n",
    "\n",
    "### DEFINE data services and software modules\n",
    "\n",
    "We make use of the International Federation Data of Seismic Networks (FDSN), the global standard and a [data service](http://www.fdsn.org/services/) for sharing seismic sensor wave form data. The Obspy librarires support FDSN. The list of resources and services that are used for retrieving station inventory and waveform data. \n",
    "1. ObsPy\n",
    "   1. FSDN as Client data sources; both (i) the FDSN client service and the (ii) FDSN complient GoeNet API webservice\n",
    "   1. Core to read data and utilize datatime features\n",
    "   1. Picking algorithms\n",
    "1. FDSN station service\n",
    "   1. retrieve station metadata information in a FDSN StationXML format or text format for all the channels in CECS station with no time limitations: https://service.geonet.org.nz/fdsnws/station/1/query?network=NZ&station=CECS&level=channel&format=text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from obspy import read_inventory\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.core import read, UTCDateTime\n",
    "#from datetime import date\n",
    "\n",
    "# Establish start and end time for retrieving waveform data\n",
    "t_start = UTCDateTime.now()-518400 #6 days ago = 60s x 60m x 24h x 6d\n",
    "t_end = UTCDateTime.now()+86400 #1 day in the future = 60s x 60m x 24h\n",
    "print('Station startime: ', t_start, '\\n & ending time: ', t_end)\n",
    "\n",
    "try:\n",
    "    #use either or GeoNet station service webservice URL or Obspy FDSN Client protocol to retrieve station data\n",
    "    st_ws = 'https://service.geonet.org.nz/fdsnws/station/1/query?network=NZ&level=station&endafter=2020-12-31&format=xml'\n",
    "    #st_ws = 'https://service.geonet.org.nz/fdsnws/station/1/query?network=NZ&station=CECS&level=channel'\n",
    "    # Set FDSN client URL to GEONET short code\n",
    "    client  = Client('GEONET')\n",
    "    print(\"Client is\",client)\n",
    "except Exception as err:\n",
    "    print(\"Error message:\", err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all Station details\n",
    "\n",
    "An initiatl step for object 1.A is determining the the types of operational seismic sensors and their locations. GoeNet hosts wave forms for a multitude of [sensor types](https://api.geonet.org.nz/network/sensor/type) (e.g. tidle guages, pressure gauges, seismometers, GNSS antennas, barometers, Microphones, Hydrophones and so on). The focus is on motion sensors of type: (i) accelerometer, (ii) broadband velocity, (iii) short period velocity, and (iv) GNSS. Furthermore, the sensors location code is unique to each sensor type. Therefore, one may chose to use the location code prefix or sensor type enumerator to select the desired sensors; i.e. seimograph and accelerometer stations. The motion sensors are used in both earthquake and volcanic seismic activity monitoring and early warning.\n",
    "\n",
    "_Sensor types that are relevant to earthquake detection are:_\n",
    "* 1 Accelerometer \n",
    "* 3 Broadband Seismometer \n",
    "* 4 GNSS Antenna \n",
    "* 8 Short Period Borehole Seismometer \n",
    "* 9 Short Period Seismometer \n",
    "* 10 Strong Motion Sensor\n",
    "\n",
    "_Location codes reserved for the seismic sensors are:_\n",
    "* 1? - weak motion sensors\n",
    "* 2? - strong motion sensors\n",
    "\n",
    "_Channel codes are:_ \n",
    "\n",
    "Defined in the GeoNet's [stream naming conventions](https://www.geonet.org.nz/data/supplementary/channels)\n",
    "First letter of the code represents a combination of sampling rate and sensor bandwidth\n",
    "\n",
    "First letter represts the sensor type \n",
    "* U (Ultra Long Period sampled at 0.01Hz, or SOH sampled at 0.01Hz)\n",
    "* V (Very Long Period sampled at 0.1Hz, or SOH sampled at 0.1Hz)\n",
    "* L (Broad band sampled at 1Hz, or SOH sampled at 1Hz)\n",
    "* B (Broad band sampled at between 10 and 80 Hz, usually 10 or 50 Hz)\n",
    "* S (Short-period sampled at between 10 and 80 Hz, usually 50 Hz)\n",
    "* H (High Broad band sampled at or above 80Hz, generally 100 or 200 Hz)\n",
    "* E (Extremely Short-period sampled at or above 80Hz, generally 100 Hz)\n",
    "\n",
    "The second letter represents the sensor type, e.g.(listed are the ones relevant to seismometers\n",
    "* H (Weak motion sensor, e.g. measuring velocity)\n",
    "* N (Strong motion sensor, e.g. measuring acceleration)\n",
    "* L (Low gain sensor, usually velocity)\n",
    "* M (Mass position, used for monitoring broadband sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "##try:\n",
    "    # st_inv = read_inventory(st_ws)\n",
    "st_inv = client.get_stations(network='NZ', location=\"1?,2?\", station='*', \n",
    "                             channel=\"UH*,VH*,LH*,BH*,SH*,HH*,EH*,UN*,VN*,LN*,BN*,SN*,HN*,EN*\",\n",
    "                             level='channel', starttime=t_start, endtime = t_end)\n",
    "\n",
    "    # Print the station inventory\n",
    "print('Number of active stations:', len(st_inv[0].stations))\n",
    "st_type_colors = {\"H\" : \"green\", \"N\" : \"orange\",\"L\" : \"blue\", \"M\" : \"yellow\"}  \n",
    "points = {\"x\":[], \"y\":[]}\n",
    "for i in range(len(st_inv[0].stations)):\n",
    "    if(st_inv[0][i].latitude < 0 and st_inv[0][i].longitude > 0):\n",
    "        print(st_inv[0].stations[i])\n",
    "        points[\"x\"].append(st_inv[0][i].latitude)\n",
    "        points[\"y\"].append(st_inv[0][i].longitude)\n",
    "\n",
    "#        st_tuple = (st_inv[0][i].code, st_inv[0][i].latitude,st_inv[0][i].longitude,\n",
    "    # Plot with basemap\n",
    "plt.figure(figsize=(30, 40))\n",
    "plt.plot(points[\"x\"],points[\"y\"], 'o', markerfacecolor='#b15928',\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "plt.title('Estimated number of Stations: %d' % len(st_inv[0].stations))\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.show()\n",
    "    # Plot network\n",
    "#st_inv.plot(projection=\"local\", resolution=\"l\", continent_fill_color='0.9', water_fill_color='1.0', \n",
    "#                   marker='*', size=50, label=False, color='#b15928', time=None, show=False, outfile=None, \n",
    "#                   method=\"basemap\", fig=None)\n",
    "plt.show()\n",
    "##except Exception as err:\n",
    "##    print(\"Error message:\", err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stataions array data preperation\n",
    "\n",
    "Prepare an array of tuples necessary and sufficient station data:\n",
    "* _station code_ as a unique identifier\n",
    "* _coordinates_ longitude & latitude\n",
    "* _elevation_ in meters above mean sea level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "st_coord = []\n",
    "\n",
    "try:\n",
    "    i = 0\n",
    "    for i in range(len(st_inv[0].stations)):\n",
    "        st_tuple = (st_inv[0][i].code, st_inv[0][i].latitude,st_inv[0][i].longitude, st_inv[0][i].elevation)\n",
    "        st_coord.append(tuple(st_tuple))\n",
    "        #st_coord = {'Code': st_inv[0][i].code,'Coordinates': {'Latitude': st_inv[0][i].latitude,'Longitude': st_inv[0][i].longitude},'Elavation': st_inv[0][i].elevation}\n",
    "#        print(st_coord[i])\n",
    "\n",
    "# Mapping stations\n",
    "except Exception as err:\n",
    "    print(\"Error message:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fault line map\n",
    "\n",
    "We have completed objective 1.A. However, we will also include a mapping of the fault lines to give a perception of the station distribution relative to that of the map of fault lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import json\n",
    "from dictor import dictor\n",
    "\n",
    "\"\"\"Extract nested values from a JSON tree.\"\"\"\n",
    "\n",
    "try:\n",
    "    with open('/home/nuwan/workspace/quasar/data/NZAFD/JSON/NZAFD_Oct_2020_WGS84.json') as json_file: \n",
    "        data = json.load(json_file)\n",
    "\n",
    "    plt.figure(figsize=(30, 40))\n",
    "\n",
    "    for each_feature in range(len(data['features'])):\n",
    "        points = {\"x\":[], \"y\":[]}\n",
    "        path = dictor(data,'features.{}.geometry.paths.0'.format(each_feature))\n",
    "        for each_coordinate in range(len(path)):\n",
    "            points[\"x\"].append(path[each_coordinate][0])\n",
    "            points[\"y\"].append(path[each_coordinate][1])\n",
    "        plt.plot(points[\"x\"],points[\"y\"], color = 'red', linewidth=2)\n",
    "\n",
    "    plt.title('Estimated number of Faults: %d' % len(data['features']))\n",
    "    plt.xlabel('Latitude')\n",
    "    plt.ylabel('Longitude')\n",
    "    plt.show()\n",
    "except Exception as err:\n",
    "    print(\"Error message:\", err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJECTIVE 1.B - STATION CLUSTERS\n",
    "\n",
    "### Cluster Stations\n",
    "\n",
    "Apply DBSCAN to cluster stations with an epsilon < 30Km. DBSCAN is preferred over K-means clustering because K-means clustering considance the variance while DBSCAN considers a distance function. It gives the capacity to build clusters serving the criteria of < 30Km distance between stations.\n",
    "\n",
    "#### Data clensing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster\n",
    "1. Apply DBSCAN\n",
    "   1. Inherent __problem of DBSCAN__ is that it characterises data points to be in the same clusted if pair-wise data points satisfy the epsilon condition. This would not adequately satisfy the required condition that all data points in a a cluster are within the desired epsilon distance.\n",
    "1. Compute the cluster property measures to estimate the acceptability\n",
    "1. Dump the output to a file including cluster label, lat/lon, station code, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "import sklearn.utils\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "def get_dbscan_labels(st_arr):\n",
    "    err=\"0\"\n",
    "    try:\n",
    "        X, labels_true = make_blobs(n_samples=len(st_arr), centers=st_arr, cluster_std=0.4,random_state=0)\n",
    "        db = DBSCAN(eps=30.0/6371.0, min_samples=3, algorithm='ball_tree', metric='haversine').fit(np.radians(X))\n",
    "        print('DBSCAN epsilon:',db.eps,'algorithm:', db.algorithm, 'metric: ', db.metric)\n",
    "        core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "        core_samples_mask[db.core_sample_indices_] = True\n",
    "        labels = db.labels_\n",
    "#        print(\"DBSCAN found %0.3f labels\" % labels )\n",
    "    except Exception as err:\n",
    "        print(\"Error message:\", err)\n",
    "        labels = \"\"\n",
    "    return labels, labels_true, core_samples_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric analysis of the clusters\n",
    "\n",
    "A large propotion of the sensors could be clustered to have, at least, 03 senors in a cluster and that they are < 30Km distance from each other; which also is the basis for the ```eps = 30.0/6371.0``` (epsilon convereted to radians using the length of Earth's radius 6371Km). The estimated number of _noise points_ tell us the number of sensors that didn't belong to any cluster. The particular geodedic data cannot be clustered with KD-Trees or any Tree algorithm. However, chosing ```algorithm = \"ball_tree\"``` is recommended as it is well suited for geospatial data clustering. The ```metric = \"haversine\"``` is naturally required to calculate the distance between two geographical points. Finally, the ```st_dbscan_arr``` comprising latitude and longitude decimal data is converted to radians to be consistent with using the _haversine_ distance funcation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all columns except the lat / lon pairs\n",
    "tmp_st_coord = np.delete(st_coord, [0,3],axis=1).astype(np.float)\n",
    "# Remove bad coordinates specific to NZ\n",
    "tmp_clean_st_coord = [item for item in tmp_st_coord if item[0] < 0 and item[1] > 0]\n",
    "# Convert to float to avoid throwing a datatype error in the plot function\n",
    "station_coordinates = np.array(tmp_clean_st_coord).astype(np.float)\n",
    "#print('Station coordinates:\\n',' '.join([str(elem) for elem in tmp_nolabel_arr])) \n",
    "\n",
    "# Run dbscan and get the labels\n",
    "labels, labels_true, core_samples_mask = get_dbscan_labels(station_coordinates)\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Total number of stations: %d' % len(labels))\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "\n",
    "# Plot a histogram of the station distribution\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(f\"Adjusted Mutual Information: %0.3f\" % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(station_coordinates, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results\n",
    "1. plot clusters with varied colors unique to each cluster\n",
    "1. plot fault lines to show closes sensor in cluster to the fault line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "plt.figure(figsize=(30, 40))\n",
    "#nz_map = Basemap(width=15000,height=15000,projection='merc',\n",
    "#            resolution='l',lat_0=-40,lon_0=176.)\n",
    "#nz_map.drawcoastlines()\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = station_coordinates[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    # uncomment to plot the noise\n",
    "    #xy = station_coordinates[class_member_mask & ~core_samples_mask]\n",
    "    #plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "    #         markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Longitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Nearest Neighbour Distance Statistics\n",
    "\n",
    "Compute the mean distance between nearest neigbours of a minimum 3 points\n",
    "* https://scikit-learn.org/stable/modules/neighbors.html\n",
    "* https://pysal.org/notebooks/explore/pointpats/distance_statistics.html#Mean-Nearest-Neighbor-Distance-Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Augment station array with cluster number\n",
    "# Start a new station coorinates and details tuple\n",
    "st_list = []\n",
    "i=0\n",
    "for i in range(len(labels)):\n",
    "    st_row = [tmp_arr[i,0],labels[i],tmp_arr[i,1],tmp_arr[i,2],tmp_arr[i,3]]\n",
    "    st_list.append(list(st_row))\n",
    "\n",
    "clusters = list({item[1] for item in st_list})\n",
    "\n",
    "for each_cluster in clusters:\n",
    "    cluster_list = list(st_list[j] for j in range(len(st_list)) if st_list[j][1] == each_cluster)\n",
    "    cluster_arr = np.delete(cluster_list, [0,1,4],axis=1).astype(np.float)\n",
    "    nbrs = NearestNeighbors(n_neighbors=3, algorithm='brute', metric='haversine').fit(cluster_arr)\n",
    "    distances, indices = nbrs.kneighbors(cluster_arr)\n",
    "    print(nbrs.kneighbors_graph(cluster_arr).toarray())\n",
    "    \n",
    "    each_cluster_clique = client.get_stations(latitude=-42.693,longitude=173.022,maxradius=30.0/6371.0, starttime = \"2016-11-13 11:05:00.000\",endtime = \"2016-11-14 11:00:00.000\")\n",
    "    print(each_cluster_clique)\n",
    "    _=inventory.plot(projection=\"local\")\n",
    "    \n",
    "    break\n",
    "\n",
    "sorted_rank = sorted(st_list, key=lambda i: (int(i[1])), reverse=True)\n",
    "#print('Code, Cluster, Latitude, Longitude, Elevation')\n",
    "#print(sorted_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of DBSCAN results\n",
    "It is evident from the cluster with large volume of data points are spread across the geography. Therefore, DBSCAN is shown to be innopriate for clustering stations to estimate whether they hold the property of being 30Km within each other.\n",
    "\n",
    "Next we "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESOURCES\n",
    "1. [Global data services and standards](http://www.fdsn.org/services/) offered by the International Federation Data of Seismic Networks (FDSN). \n",
    "1. GEONET resources:\n",
    "   1. [Stream Naming Conventions](https://www.geonet.org.nz/data/supplementary/channels) are based on historical usage together with recommendations from the [SEED manual](https://www.fdsn.org/seed_manual/SEEDManual_V2.4.pdf)\n",
    "   1. [Python tutorials](https://www.geonet.org.nz/data/tools/Tutorials) for using GeoNet resources\n",
    "1. [Seismo-Live](https://krischer.github.io/seismo_live_build/html/Workshops/2017_Baku_STCU_IRIS_ObsPy_course/07_Basic_Processing_Exercise_solution_wrapper.html) examples of get station waveform, inventory, event, arrival time, response, and plotting using obspy\n",
    "1. Choosing [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) over KMeans: \n",
    "   1. Fundermentally KMeans requires us to first select the number of clusters we wish to find and DBSCAN doesn't.\n",
    "   1. [clustering to reduce spatial data sizes](https://geoffboeing.com/2014/08/clustering-to-reduce-spatial-data-set-size/) KMeans is not an ideal algorithm for latitude-longitude spatial data because it minimizes variance, not geodetic distance. \n",
    "   1. [Explanation of DBSCAN clustering](https://towardsdatascience.com/explaining-dbscan-clustering-18eaf5c83b31) also identifies a drawback of KMeans clustering as it is vulnerable to outliers and outliers have a significant impact on the way the centroids moves.\n",
    "1. [Example of scikit-learn DBSCAN](https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html)\n",
    "1. [obspy.geodetics](https://docs.obspy.org/packages/obspy.geodetics.html) - various geodetic utilities for ObsPy - try an alternative clustering method with obspy geodetics\n",
    "1. Mapping tutorials\n",
    "   1. Visualization: [Mapping Global Earthquake Activity](http://introtopython.org/visualization_earthquakes.html)\n",
    "   1. Plotting data on a map [(Example Gallery)](https://matplotlib.org/basemap/users/examples.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
